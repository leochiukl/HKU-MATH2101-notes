\section{Vector Spaces}
\label{sect:vector-spaces}
\subsection{Vector Spaces and Vector Subspaces}
\begin{enumerate}
\item In \Cref{sect:vector-spaces}, we will start studying a main object of
interest in linear algebra: \emph{vector space}, which \emph{abstractizes} the
properties we know in \(\R^n\).
\item A \defn{vector space} over \(\R\) (or simply \emph{real} vector space) is
a nonempty set \(V\) on which two operations, called \defn{vector addition} and
\defn{scalar multiplication} are defined such that for any elements
\(\vect{u},\vect{v},\vect{w}\in V\) (called \defn{vectors} in \(V\)) and any
scalars \(a,b\in\R\), (i) the sum \(\vect{u}+\vect{v}\) and the scalar multiple
\(a\vect{v}\) are unique elements of \(V\), and (ii) the following axioms hold.
\begin{enumerate}[label={(\arabic*)}]
\item (commutativity) \(\vect{v}+\vect{u}=\vect{u}+\vect{v}\).
\item (associativity) \((\vect{v}+\vect{u})+\vect{w}=\vect{u}+(\vect{v}+\vect{w})\).
\item (zero vector) There exists an element \(\vect{0}\in V\) such that \(\vect{v}+\vect{0}=\vect{v}\).
\item (additive inverse) There exists an element \(\vc{-\vect{v}}\in V\) such that
\(\vect{v}+\vc{(-\vect{v})}=\vect{0}\).
\item \(1\vect{v}=\vect{v}\).
\item \((ab)\vect{v}=a(b\vect{v})\).
\item \(a(\vect{u}+\vect{v})=a\vect{u}+a\vect{v}\).
\item \((a+b)\vect{v}=a\vect{v}+b\vect{v}\).
\end{enumerate}
\begin{note}
In general, we can define a vector space over a \emph{field} \(K\), where the
scalars are elements of the \emph{field} \(K\). But here we shall focus on real
vector spaces. See MATH2102 for more details about this more general notion of
vector space.
\end{note}
\item With these axioms, we can deduce the following ``natural'' properties.
\begin{proposition}
\label{prp:vector-space-prop}
Let \(\vect{v}\) be a vector in a vector space \(V\). Then, the following hold.
\begin{enumerate}
\item The zero vector \(\vect{0}\) in axiom (3) is unique.
\item The scalar multiple \(0\vect{v}\) is the zero vector.
\item The additive inverse \(-\vect{v}\) in axiom (4) equals \((-1)\vect{v}\)
(and thus is uniquely determined by \(\vect{v}\)).
\item For any \(a\in\R\), we have \(a\vect{0}=\vect{0}\).
\end{enumerate}
\end{proposition}
\begin{pf}
\begin{enumerate}
\item Assume that there are two zero vectors \(\vect{0}\) and \(\vect{0}'\)
satisfying axiom (3). Then, by applying axioms (1) and (3), we have
\[
\vect{0}'\overset{\text{(3)}}{=}\vect{0}'+\vect{0}\overset{\text{(1)}}{=}
\vect{0}+\vect{0}\overset{\text{(3)}}{=}\vect{0}.
\]
\item Using the axioms (5) and (8), we have
\[
0\vect{v}+\vect{v}\overset{\text{(5)}}{=}0\vect{v}+1\vect{v}
\overset{\text{(8)}}{=}(0+1)\vect{v}
=1\vect{v}
\overset{\text{(5)}}{=}\vect{v}.
\]
Adding additive inverse \(-\vect{v}\) on both sides, we get
\[
(0\vect{v}+\vect{v})+(-\vect{v})=\vect{v}+(-\vect{v})
\overset{\text{(2), (4)}}{\implies}
0\vect{v}+\vect{0}=\vect{0}
\overset{\text{(3)}}{\implies}
0\vect{v}=\vect{0}.
\]
\item Consider:
\begin{align*}
\vect{v}+(-\vect{v})&=\vect{0}&\text{(4)}\\
\implies(-1)\vect{v}+[\vect{v}+(-\vect{v})]&=(-1)\vect{v}+\vect{0}\\
\implies[(-1)\vect{v}+\vect{v}]+(-\vect{v})&=(-1)\vect{v}&\text{(2), (3)}\\
\implies[(-1)\vect{v}+1\vect{v}]+(-\vect{v})&=(-1)\vect{v}&\text{(5)}\\
\implies 0\vect{v}+(-\vect{v})&=(-1)\vect{v}&\text{(8)}\\
\implies -\vect{v}&=(-1)\vect{v}&\text{(b), (1), (3)}.
\end{align*}
\item Setting \(\vect{v}=\vect{0}\) in (b), we have \(0\vect{0}=\vect{0}\).
Hence,
\[
a\vect{0}=a(0\vect{0})=(a0)\vect{0}=0\vect{0}=\vect{0}.
\]
\end{enumerate}
\end{pf}
\item Some examples of vector space (the vector addition and scalar
multiplication are naturally defined in the usual way for the following):
\begin{enumerate}
\item The set of \(\R^n\) of column vectors.
\item The set of \(m\times n\) matrices, denoted by \defn{\(M_{m\times n}(\R)\)}.
\begin{note}
The zero vector in this vector space is the \emph{zero matrix} \(O_{m\times n}\).
\end{note}
\item The set of all functions from \(\R\) to \(\R\).
\begin{note}
The zero vector in this vector space is the constant zero function, i.e.,
function \(f:\R \to\R\) defined by \(f(x)=0\) for any \(x\in\R\).
\end{note}
\end{enumerate}
\item Just like a set can have \emph{subsets}, a vector space can have (vector)
\emph{subspaces}. The idea is that we want to extract a ``part'' of a vector
space such that the part is still a valid vector space on its own.

\item Let \(V\) be a vector space. A nonempty subset \(W\) of \(V\) is a
\defn{vector subspace} (or simply \defn{subspace}) of \(V\) if for any
\(\vect{u},\vect{v}\in W\) and any scalar \(a\in\R\),
\begin{enumerate}
\item (closed under addition) \(\vect{u}+\vect{v}\in W\).
\item (closed under scalar multiplication) \(a\vect{v}\in W\).
\end{enumerate}
\item The following property is useful for showing that a given set is
\emph{not} a vector subspace.
\begin{proposition}
\label{prp:vec-subsp-contain-zero}
Let \(W\) be a vector subspace of a vector space \(V\). Then, the zero vector
\(\vect{0}\) in \(V\) belongs also to \(W\).
\end{proposition}
\begin{pf}
Consider any \(\vect{v}\in W\subseteq V\), which exists since \(W\) is
nonempty. Then note that
\[
\vect{0}=-\vect{v}+\vect{v}=(-1)\vect{v}+\vect{v}\in W
\]
where the last ``\(\in\)'' follows from the closedness under addition and
scalar multiplication.
\end{pf}

From this result, we know that if a subset \(W\) of a vector space \(V\) does
\emph{not} contain the zero vector \(\vect{0}\) in \(V\), then it cannot
possibly be a vector subspace.

\item The following result justifies that a vector subspace is indeed a vector
space.
\begin{proposition}
\label{prp:vec-subsp-is-vec-sp}
Let \(W\) be a vector subspace of a vector space \(V\). Then \(W\) is also a
vector space, with the vector addition and scalar multiplication from \(V\).
\end{proposition}
\begin{pf}
Firstly, closedness under addition and scalar multiplication guarantees that
vector addition and scalar multiplication must yield unique elements in \(W\).
So it remains to check the axioms of vector space. In fact, we only need to
check axioms (3) and (4) since other axioms must hold due to the fact that
\(W\subseteq V\) and \(V\) is a vector space.

Axiom (3) follows from \Cref{prp:vec-subsp-contain-zero}. Axiom (4) holds by
noting that, for any \(\vect{v}\in W\), there exists \((-1)\vect{v}\in W\) such
that
\[
\vect{v}+(-1)\vect{v}=(1-1)\vect{v}=0\vect{v}=\vect{0}.
\]
\end{pf}
\item Some examples of vector subspaces:
\begin{enumerate}
\item A vector space \(V\) is always a vector subspace of itself.
\item Given any vector space \(V\), the subset of \(V\) containing only the
zero vector, i.e., \(W=\{\vect{0}\}\) is a vector subspace of \(V\). It is
called the \defn{zero subspace} of \(V\).
\item The set of all diagonal \(n\times n\) matrices is a vector subspace of the
vector space \(M_{n\times n}(\R)\).
\item The set of all (real) polynomials is a vector subspace of the vector
space of all functions from \(\R\) to \(\R\).
\end{enumerate}
\end{enumerate}
\subsection{Spanning Sets}
\begin{enumerate}
\item Consider a fixed vector \(\vect{v}\in\R^n\). Note that the set
\(\{c\vect{v}:c\in\R\}\) is a vector subspace of \(\R^n\). Here, we use a
single vector to ``yield''  a vector subspace.

Next, consider two vectors \(\vect{i}\) and \(\vect{j}\) in \(\R^2\). Note that
the set \(\{a\vect{i}+b\vect{j}:a,b\in\R\}\) is indeed just \(\R^2\). So it
turns out that the whole vector space \(\R^2\) can be obtained by using just
two vectors. They may be seen as ``cores'' of \(\R^2\).

\item We have used the notion of \emph{linear combination} above, which is
defined in \labelcref{it:lin-comb} in the context of vector space \(\R^n\).
Here, we will define it more generally as follows.

Let \(S\) be a nonempty subset of \(V\). A vector \(\vect{v}\in V\) is a
\defn{linear combination} of vectors in \(S\) if there exist a \emph{finite}
number of vectors \(\vect{v}_1,\dotsc,\vect{v}_{n}\in S\) and scalars
\(a_1,\dotsc,a_n\in\R\) such that
\[
\vect{v}=a_1\vect{v}_1+\dotsb+a_n\vect{v}_n.
\]
In such case, we call \(\vect{v}\) as a \defn{linear combination} of vectors
\(\vect{v}_1,\dotsc,\vect{v}_n\).

\item Some examples of linear combination:
\begin{enumerate}
\item In \(\R^3\), \(\mqty[1\\2\\3]\) is a linear combination of
\(\mqty[1\\0\\0]\), \(\mqty[0\\1\\0]\), and \(\mqty[0\\0\\1]\).
\item In \(M_{2\times 2}(\R)\), \(\mqty[1&3\\ 2&0]\) is a linear combination of
\(\mqty[1&3\\ 0&0]\), \(\mqty[0&0\\ 2&-1]\), and \(\mqty[0&0\\ 2&1]\) since
\[
\mqty[1&3\\ 2&0]=\mqty[1&3\\ 0&0]+\frac{1}{2}\mqty[0&0\\ 2&-1]+\frac{1}{2}\mqty[0&0\\ 2&1].
\]
\item In the vector space of all polynomials, \(1+2x-6x^5\) is a linear
combination of \(1+3x\) and \(x+6x^5\) since
\[
1+2x-6x^5=(1+3x)-(x+6x^5).
\]
\end{enumerate}
\item Let \(S\) be a nonempty subset of a vector space \(V\). The \defn{span}
of \(S\), denoted by \(\spn{S}\), is the set of all linear
combinations of the vectors in \(S\), i.e.,
\[
\spn{S}=\qty{\sum_{i=1}^{k}a_i\vect{v}_i:a_1,\dotsc,a_k\in\R, \vect{v}_1,\dotsc,\vect{v}_k\in S, k\in\N}
\]
Conventionally, we define \(\spn{\varnothing}=\{\vect{0}\}\). \begin{intuition}
The span of empty set, \(\spn{\varnothing}\) is a set including only an ``empty
sum'', which should naturally be \(\vect{0}\): The sum of a vector and an
``empty sum'' should still be that vector.
\end{intuition}

\item It turns out that ``spanning'' always results in a vector subspace.
\begin{proposition}
\label{prp:span-vec-subsp}
Let \(V\) be a vector space and \(S\) be any subset of \(V\). Then, \(\spn{S}\)
is a vector subspace of \(V\).
\end{proposition}
\begin{pf}
If \(S=\varnothing\), then there is nothing to prove as
\(\spn{S}=\{\vect{0}\}\) is simply the zero subspace in this case.
Henceforth assume that \(S\ne\varnothing\). Fix any
\(\vect{u},\vect{v}\in\spn{S}\). Then we can write
\[
\vect{u}=a_1\vect{u}_1+\dotsb+a_r\vect{u}_r,
\]
for some \(a_1,\dotsc,a_r\in\R\) and \(\vect{u}_1,\dotsc,\vect{u}_r\in S\).
Also, we can write
\[
\vect{v}=b_1\vect{v}_1+\dotsb+b_s\vect{v}_s,
\]
for some \(b_1,\dotsc,b_s\in\R\) and \(\vect{v}_1,\dotsc,\vect{v}_s\in S\).

Now, it remains to check that \(\vect{u}+\vect{v}\in\spn{S}\) and
\(c\vect{v}\in\spn{S}\) for any \(c\in\R\).

\underline{\(\vect{u}+\vect{v}\in\spn{S}\)}: Note that
\[
\vect{u}+\vect{v}
=(a_1\vect{u}_1+\dotsb+a_r\vect{u}_r)+(b_1\vect{v}_1+\dotsb+b_s\vect{v}_s)
\]
is a linear combination of vectors
\(\vect{u}_1,\dotsc,\vect{u}_r,\vect{v}_1,\dotsc,\vect{v}_s\) in \(S\). Thus,
\(\vect{u}+\vect{v}\in\spn{S}\).

\underline{\(c\vect{v}\in\spn{S}\) for any \(c\in\R\)}: Note that for any
\(c\in\R\),
\[
c\vect{v}=c(b_1\vect{v}_1+\dotsb+b_s\vect{v}_s)
=(cb_1)\vect{v}_1+\dotsb+(cb_s)\vect{v}_s
\]
is a linear combination of vectors in \(S\), hence \(c\vect{v}\in\spn{S}\).
\end{pf}
\item A special kind of vector subspace obtained by spanning is the
\emph{column space}, which is related to spanning the \emph{columns} of a
matrix.

Let \(A\) be an \(m\times n\) matrix and let \(\vect{v}_1,\dotsc,\vect{v}_n\)
be the columns of \(A\). Then,
\[
\col{A}=\spn{\{\vect{v}_1,\dotsc,\vect{v}_n\}}
\]
is called the \defn{column space} of \(A\). 

\item\label{it:col-space-expr} By definition of matrix-vector product, we can
show that
\[
\col{A}=\{A\vect{x}\in\R^n:\vect{x}\in\R^m\}.
\]
\begin{pf}
``\(\subseteq\)'': Fix any \(\vect{u}\in\col{A}\). Then, we have
\[
\vect{u}=x_1\vect{v}_1+\dotsb+x_n\vect{v}_n
\]
for some scalars \(x_1,\dotsc,x_n\in\R\). But then we can just express it as
\[
\vect{u}=A\vect{x}
\]
where \(\vect{x}=\mqty[x_1&\cdots& x_n]^{T}\in\R^m\) by definition of matrix-vector product.

``\(\supseteq\)'': Fix any \(\vect{u}\in\{A\vect{x}\in\R^n:\vect{x}\in\R^m\}\).
Then, \(\vect{u}=A\vect{x}\) for some \(\vect{x}\in\R^m\). Then, applying the
definition of matrix-vector product again, writing
\(\vect{x}=\mqty[x_1&\cdots &x_n]^{T}\), we have
\[
\vect{u}=x_1\vect{v}_1+\dotsb+x_n\vect{v}_n,
\]
which means that \(\vect{u}\in\spn{\{\vect{v}_1,\dotsc,\vect{v}_n\}}\).
\end{pf}
\item A subset \(S\) of a vector space \(V\) is said to \defn{span} \(V\), or
is a \defn{spanning set} of \(V\) if \(\spn{S}=V\).

Since every linear combination of vectors in \(S\subseteq V\) lies in \(V\) by
the definition of vector space, we always have \(\spn{S}\subseteq V\). Hence,
to show that \(S\) spans \(V\), it actually suffices to show that
\(\spn{S}\supseteq V\), i.e., \emph{every vector in \(V\) is a linear
combination of vectors in \(S\).}

\item It turns that the concept of \emph{solving system of linear equations} we
have studied in \Cref{sect:sle} is useful for showing this.

For example, consider \(V=\R^m\) and
\(S=\{\vect{v}_1,\dotsc,\vect{v}_n\}\subseteq V\). Then, for any
\(\vect{u}=\mqty[u_1&\cdots&u_m]^{T}\in\R^m\), we need to investigate whether
the following equation, with unknowns \(x_1,\dotsc,x_n\), has any solution:
\[
x_1\vect{v}_1+\dotsb+x_n\vect{v}_n=\vect{u}.
\]
Writing \(\vect{v}_j=\mqty[v_{1j}\\\vdots\\v_{mj}]\) for any \(j=1,\dotsc,n\),
we can write the equation above more explicitly as a system of \(m\) linear
equations in \(n\) unknowns:
\[
\begin{cases}
v_{11}x_1+\dotsb+v_{1n}x_n=u_1,\\
v_{21}x_1+\dotsb+v_{2n}x_n=u_2,\\
\qquad\vdots\\
v_{m1}x_1+\dotsb+v_{mn}x_n=u_m.\\
\end{cases}
\]
Its augmented matrix is
\[
\left[\begin{array}{@{}cccc|c@{}}
v_{11}&v_{12}&\cdots&v_{1n}&u_{1}\\
v_{21}&v_{22}&\cdots&v_{2n}&u_{2}\\
\vdots&\vdots&\ddots&\vdots&\vdots\\
v_{m1}&v_{m2}&\cdots&v_{mn}&u_{m}\\
\end{array}\right].
\]
Then, we know that \(S\) spans \(V\) iff the system is \emph{consistent}. So
methods for determining consistency in \Cref{subsect:sys-sol-num} can be
utilized to determine whether \(S\) spans \(V\) or not.
\end{enumerate}
\subsection{Linear Independence}
\begin{enumerate}
\item While \emph{spanning set} is about the \emph{existence} of solution to a
system, \emph{linear independence} is about the \emph{uniqueness} of solution
to a system.

\item A subset \(S\) of a vector space \(V\) is called \defn{linearly
dependent} if there exist distinct vectors \(\vect{v}_1,\dotsc,\vect{v}_n\in
S\), and scalars \(a_1,\dotsc,a_n\in\R\) which are \emph{not all zero} (for
some \(n\in\N\)) such that
\[
a_1\vect{v}_1+\dotsb+a_n\vect{v}_n=\vect{0}.
\]
\begin{intuition}
This means that we can express a vector in \(S\) as a linear combination of
some other vectors in the same set \(S\), so some vectors in \(S\) are
``related linearly'' \faIcon{arrow-right} ``linearly dependent''.

The equality would hold if the scalars were all zero, \emph{regardless} of what
\(S\) is. So we would like to exclude this ``boring'' case.
\end{intuition}

\item A subset \(S\) of a vector space \(V\) is called \defn{linearly
independent} if \(S\) is \emph{not} linearly dependent, i.e., for any
\(n\in\N\), the only scalars \(a_1,\dotsc,a_n\in\R\) satisfying
\[
a_1\vect{v}_1+\dotsb+a_n\vect{v}_n=\vect{0},\quad\text{where
\(\vect{v}_1,\dotsc,\vect{v}_n\in S\) are distinct},
\]
are the trivial one: \(a_1=\dotsb=a_n=0\), i.e., symbolically,
\[
a_1\vect{v}_1+\dotsb+a_n\vect{v}_n=\vect{0}
\implies a_1=\dotsb=a_n=0.
\]
In other words, for any finitely
many distinct \(\vect{v}_1,\dotsc,\vect{v}_n\in S\), the homogeneous system
(with unknowns \(a_1,\dotsc,a_n\))
\[
\mqty[\vect{v}_1&\cdots&\vect{v}_n]\mqty[a_1\\\vdots\\ a_n]=\mqty[0\\\vdots\\ 0]
\]
only has the trivial solution \(a_1=\dotsb=a_n=0\). Since a homogeneous system
always has the trivial solution, this is equivalent to saying that the
homogeneous system has a \emph{unique} solution. So again the methods discussed
in \Cref{subsect:sys-sol-num} are useful for determining linear independence.

\begin{note}
Writing \(S=\{\vect{v}_1,\dotsc,\vect{v}_n\}\), sometimes we say
``\(\vect{v}_1,\dotsc,\vect{v}_n\) are linearly (in)dependent'' to mean
``\(S\) is linearly (in)dependent''.
\end{note}

\item \label{it:two-nonzero-vec-lin-dep}
We can have a more ``intuitive'' equivalent definition for linear
dependence when we focus on a set of two nonzero vectors.

Consider a subset \(S=\{\vect{u},\vect{v}\}\) of a vector space \(V\), where
\(\vect{u}\) and \(\vect{v}\) are nonzero. Then \(S\) is linearly dependent iff
\(\vect{u}=a\vect{v}\) for some \(a\ne 0\).

\begin{pf}
``\(\Leftarrow\)'': Assume that \(\vect{u}=a\vect{v}\) for some \(a\ne 0\).
Then, we can write
\[
(1)\vect{u}+(-a)\vect{v}=\vect{0},
\]
so \(S\) is linearly dependent.

``\(\Rightarrow\)'': Assume that \(S\) is linearly dependent. Then there exist
scalars \(a_1,a_2\in\R\), not all zero, such that
\[
a_1\vect{u}+a_2\vect{v}=\vect{0}.
\]
We claim that \(a_1\ne 0\). To prove this, assume to the contrary that
\(a_1=0\). Then, we have \(a_2\ne 0\) and also \(a_2\vect{v}=\vect{0}\). This
implies \(\vect{v}=\vect{0}\), contradiction. Similarly, we can show that
\(a_2\ne 0\).

Hence, we can write
\[
\vect{u}=\underbrace{\frac{-a_2}{a_1}}_{\ne 0}\vect{v}
\]
as desired.
\end{pf}
\item \label{it:lin-dep-subsets}
Let \(V\) be a vector space and let \(S_1\subseteq S_2\subseteq V\). Then, the
following hold.
\begin{enumerate}
\item If \(S_2\) is linearly independent, then \(S_1\) is also linearly independent.
\item If \(S_1\) is linearly dependent, then \(S_2\) is also linearly
dependent.
\end{enumerate}
\begin{pf}
\begin{enumerate}
\item Assume that \(S_2\) is linearly independent. Then for any \(n\in\N\) and
any distinct vectors \(\vect{v}_1,\dotsc,\vect{v}_n\in S_2\), we have
\[
a_1\vect{v}_1+\dotsb+a_n\vect{v}_n=\vect{0}
\implies a_1=\dotsb=a_n=0.
\]
But as \(S_1\subseteq S_2\), this implication must also hold for arbitrary
distinct vectors in \(S_1\).

\item The statement is just the contrapositive of (a), and hence is true also.
\end{enumerate}
\end{pf}

However, the converse does \emph{not} hold in general. WLOG, we focus on the
converse of (a): ``If \(S_1\) is linearly independent, then \(S_2\) is linearly
independent.'' Take any \(S_1\subseteq V\) which is linearly independent and
take \(S_2=S_1\cup\{\vect{0}\}\).  Then, \(S_2\supseteq S_1\) but \(S_2\) is
always linearly dependent.
\end{enumerate}
\subsection{Basis}
\begin{enumerate}
\item The next concept to be discussed is \emph{basis}, which neatly
``marries'' the concept of \emph{spanning set} and \emph{linear independence}.
A \defn{basis} for a vector space \(V\) is a \emph{linearly independent} subset
of \(V\) which also \emph{spans} \(V\).

\begin{intuition}
A basis can be seen as a ``minimal'' spanning set for \(V\), without
``redundancy''.
\end{intuition}

\item To illustrate why a basis can be seen as
``minimal'' spanning set for \(V\), consider the following results. Let
\(\beta\) be a basis for a vector space \(V\).
\begin{enumerate}
\item \label{it:span-set-not-smaller-than-basis} If \(S\) is a proper subset of \(\beta\), then \(\spn{S}\ne V\).

\begin{pf}
Assume that \(S\) is a proper subset of \(\beta\). Then, there exists
\(\vect{v}\in\beta\subseteq V\) with \(\vect{v}\notin S\). Now, suppose on the
contrary that \(\vect{v}\in\spn{S}\). Then, we can write
\[
\vect{v}=a_1\vect{v}_1+\dotsb+a_n\vect{v}_n
\]
where \(a_1,\dotsc,a_n\in\R\), \(v_1,\dotsc,v_n\in S\), and \(n\in\N\).
This implies that
\[
a_1\vect{v}_1+\dotsb+a_n\vect{v}_n+(-1)\vect{v}=\vect{0},
\]
thus \(\{\vect{v}_1,\dotsc,\vect{v}_n,\vect{v}\}\subseteq \beta\) is \emph{not}
linearly independent, which would imply that \(\beta\) is \emph{not} linearly
independent also by \labelcref{it:lin-dep-subsets}, contradiction. This means
\(\vect{v}\notin\spn{S}\), so we have found an element in \(V\) \emph{not} in
\(\spn{S}\), meaning that \(\spn{S}\ne V\).
\end{pf}

\begin{intuition}
This means that there is not ``smaller'' spanning set for \(V\) than the
``minimal'' one.
\end{intuition}
\item \label{it:span-set-greater-than-basis-ld} If \(\beta\) is a proper subset of a set \(S\subseteq V\), then \(S\) is
not linearly independent.

\begin{pf}
Assume that \(\beta\) is a proper subset of \(S\subseteq V\). Then there exists
\(\vect{v}\in S\subseteq V\) with \(\vect{v}\notin \beta\). Since
\(\beta=\{\vect{b}_1,\dotsc,\vect{b}_n\}\) spans \(V\), we can write
\[
\vect{v}=a_1\vect{b}_1+\dotsb+a_n\vect{b}_n
\]
for some scalars \(a_1,\dotsc,a_n\in\R\). But since \(\beta\) is a proper
subset of \(S\), we have \(S\supseteq
\{\vect{b}_1,\dotsc,\vect{b}_n,\vect{v}\}\). By writing the equation above as
\[
a_1\vect{b}_1+\dotsb+a_n\vect{b}_n+(-1)\vect{v}=\vect{0},
\]
we note that the latter set \(\{\vect{b}_1,\dotsc,\vect{b}_n,\vect{v}\}\) is
\emph{not} linearly independent, so do \(S\) (by \labelcref{it:lin-dep-subsets}).
\end{pf}
\begin{intuition}
Note that \(S\) also spans \(V\) since \(\beta\) is a proper subset of \(S\).
However, as a set ``larger'' than the ``minimal'' spanning set, it would
contain some ``redundancy'' \faIcon{arrow-right} not linearly independent.
\end{intuition}
\end{enumerate}
\item Consider the vector space \(\R^n\). A standard example of basis of
\(\R^n\) is the set \(\beta=\{\vect{e}_1,\dotsc,\vect{e}_n\}\), where the
elements are the standard vectors. It is called the \defn{standard basis} for
\(\R^n\).

\begin{pf}
We can prove that the standard basis is indeed a basis for \(\R^n\).

Firstly, we show that it spans \(\R^n\). For any
\(\vect{v}=\mqty[v_1&\cdots&v_n]^{T}\in\R^n\), we can write
\[
\vect{v}=v_1\vect{e}_1+\dotsb+v_n\vect{e}_n,
\]
so \(\vect{v}\in\spn{\beta}\). This implies that \(\spn{\beta}=\R^n\).

Next, to show the linear independence of \(\beta\), consider first the equation
\[
x_1\vect{e}_1+\dotsb+x_n\vect{e}_n=\vect{0}.
\]
It can be rewritten as a homogeneous system
\[
I_n\mqty[x_1\\ \vdots\\ x_n]=\mqty[0\\ \vdots\\ 0],
\]
which clearly has the unique solution \(x_1=\dotsb=x_n=0\).
\end{pf}
\item Note that basis is \emph{not unique}. For example, for the vector space
\(\R^2\), two possible bases are the standard basis \(\{\vect{i},\vect{j}\}\),
and the set \(\{2\vect{i},2\vect{j}\}\).

\item Let \(V\) be a vector space and \(S\) be any linearly independent subset
of \(V\). Then, \(S\) is a basis for its span \(\spn{S}\).

\begin{pf}
Linear independence of \(S\) follows from assumption, and by definition \(S\)
must be a spanning set of \(\spn{S}\), where \(\spn{S}\) is considered as a
vector space.
\end{pf}

\item The main result about \emph{basis} is the following.
\begin{theorem}
\label{thm:basis-unique-lin-comb}
Let \(V\) be a nonzero vector space and \(\beta\) be a subset of \(V\). Then,
\(\beta\) is a basis for \(V\) iff for any vector \(\vect{u}\in V\),
\(\vect{u}\) can be uniquely written as a linear combination of vectors in
\(\beta\), i.e.,
\[
\vect{u}=a_1\vect{v}_1+\dotsb+a_n\vect{v}_n
\]
for some \emph{unique} scalars \(a_1,\dotsc,a_n\in\R\), with
\(\beta=\{\vect{v}_1,\dotsc,\vect{v}_n\}\).
\end{theorem}
\begin{intuition}
The spanning property of basis corresponds to the \emph{existence} of such
linear combination, and the linear independence of basis corresponds to the
\emph{uniqueness} of such linear combination.
\end{intuition}

\begin{pf}
``\(\Rightarrow\)'': Assume that \(\beta\) is a basis for \(V\), and fix any
vector \(\vect{u}\in V\).

\underline{Existence}: Since \(\beta\) spans \(V\), there exist scalars
\(a_1,\dotsc,a_n\in\R\) such that
\[
\vect{u}=a_1\vect{v}_1+\dotsb+a_n\vect{v}_n.
\]

\underline{Uniqueness}: Suppose that we can also write
\(\vect{u}=b_1\vect{v}_1+\dotsb+b_n\vect{v}_n\) for some scalars
\(b_1,\dotsc,b_n\). Then, we have
\[
\vect{u}=a_1\vect{v}_1+\dotsb+a_n\vect{v}_n=b_1\vect{v}_1+\dotsb+b_n\vect{v}_n,
\]
which implies
\[
(a_1-b_1)\vect{v}_1+\dotsb+(a_n-b_n)\vect{v}_n=\vect{0}.
\]
By linear independence of \(\beta\), it follows that
\[
a_1-b_1=\dotsb=a_n-b_n=0,
\]
which means \(a_1=b_1,\dotsc,a_n=b_n\).

``\(\Leftarrow\)'': Assume that for any \(\vect{u}\in V\), we can write
\[
\vect{u}=a_1\vect{v}_1+\dotsb+a_n\vect{v}_n
\]
for some \emph{unique} scalars \(a_1,\dotsc,a_n\in\R\), with
\(\beta=\{\vect{v}_1,\dotsc,\vect{v}_n\}\). This immediately shows that
\(\spn{\beta}=V\). So it remains to prove that \(\beta\) is linearly
independent. Consider the equation
\[
c_1\vect{v}_1+\dotsb+c_n\vect{v}_n=\vect{0}=0\vect{v}_1+\dotsb+0\vect{v}_n.
\]
By the uniqueness, we must have \(c_1=\dotsb=c_n=0\), as desired.
\end{pf}
\end{enumerate}
\subsection{Dimensions}
\label{subsect:dim}
\begin{enumerate}
\item From now on, we shall focus on vector spaces with \emph{finite} bases
unless otherwise specified. Vector spaces with \emph{infinite} bases are
studied in \emph{functional analysis}, which is a much more advanced topic.
(See MATH4404 for more details.)

\item In \Cref{subsect:dim}, we shall discuss a ``famous'' concept:
\emph{dimension}. Most of us should have some intuitive idea about it, but here
we will try to formalize this notion mathematically (in the context of linear
algebra).

\item Before discussing the concept of dimension, we shall first consider
several preliminary results.

\begin{proposition}
\label{prp:li-num-leq-dim}
Let \(\beta\) be a basis for a vector space \(V\). If \(\beta'\) is a
linearly independent subset of \(V\), then \(|\beta'|\le |\beta|\). (Here
\(|\cdot|\) denotes cardinality.)
\end{proposition}
\begin{pf}
Firstly, if \(V\) is the zero vector space \(\{\vect{0}\}\), this result is
trivial since the only linearly independent subset of \(V\) is the empty set
\(\varnothing\). Thus, henceforth we suppose that \(V\) is a nonzero vector
space.

Let \(m=|\beta|\) and \(n=|\beta'|\). Then, we write
\(\beta=\{\vect{v}_1,\dotsc,\vect{v}_m\}\) and
\(\beta'=\{\vect{u}_1,\dotsc,\vect{u}_n\}\).

Since \(\beta\) spans \(V\) and \(\beta'\) is a subset of \(V\), for any
\(j=1,\dotsc,n\), we can write
\[
\vect{u}_j=a_{1j}\vect{v}_1+\dotsb+a_{mj}\vect{v}_m
\]
for some scalars \(a_{1j},\dotsc,a_{mj}\).

Now, we set
\[
x_1\vect{u}_1+\dotsb+x_n\vect{u}_n=0,
\]
which can be rewritten as
\[
\sum_{i=1}^{m}(a_{i1}x_1+\dotsb+a_{in}x_n)\vect{v}_i=0.
\]
By linear independence of \(\beta=\{\vect{v}_1,\dotsc,\vect{v}_m\}\), we
obtain the following homogeneous system of linear equations
\[
\begin{cases}
a_{11}x_{1}+\dotsb+a_{1n}x_{n}=0\\
a_{21}x_{1}+\dotsb+a_{2n}x_{n}=0\\
\hspace{2cm}\vdots\\
a_{m1}x_1+\dotsb+a_{mn}x_n=0.
\end{cases}
\]
Now, assume to the contrary that \(n>m\). In such case, by
\Cref{prp:homo-var-more-than-eqs-inf}, this system would have a non-trivial
solution, contradicting to the linear independence of
\(\beta'=\{\vect{u}_1,\dotsc,\vect{u}_n\}\).
\end{pf}

\item
Using \Cref{prp:li-num-leq-dim}, we can show the following relationship between
any two bases for a vector space \(V\).

\begin{corollary}
\label{cor:two-bases-same-num}
Let \(\beta_1\) and \(\beta_2\) be any two bases of a vector space \(V\). Then,
\(\beta_1\) and \(\beta_2\) contain the same number of vectors.
\end{corollary}

\begin{pf}
Setting \(\beta=\beta_1\) and \(\beta'=\beta_2\) in \Cref{prp:li-num-leq-dim}
gives \(|\beta_2|\le|\beta_1|\). On the other hand, setting \(\beta=\beta_2\)
and \(\beta'=\beta_1\) in \Cref{prp:li-num-leq-dim} gives
\(|\beta_1|\le|\beta_2|\). Hence, we have \(|\beta_1|=|\beta_2|\).
\end{pf}

\item From \Cref{cor:two-bases-same-num}, we see that the \emph{cardinality of
basis} is independent from the choice of basis. It only depends on the
underlying vector space. This suggests the notion of \emph{dimension}. The
number of vectors in a basis for a vector space \(V\) is called the
\defn{dimension} of \(V\), denoted by \(\dim(V)\).

\begin{note}
Roughly speaking, dimension may be seen as the number of ``degrees of
freedom'' or ``independent parameters''.
\end{note}

Example: The dimension of \(\R^n\) is \(n\) (as expected) since the standard
basis \(\{\vect{e}_1,\dotsc,\vect{e}_n\}\) of \(\R^n\) has \(n\) vectors.

\item To close \Cref{subsect:dim}, we give some results regarding dimension.
\begin{enumerate}
\item \label{it:dim-span} Let \(S\) be a finite linearly independent subset of
a vector space \(V\). Then, \(\dim(\spn{S})=|S|\).

\begin{pf}
Since \(S\) is a spanning set of \(\spn{S}\) (where \(\spn{S}\) is considered
as a vector space) and \(S\) is linearly independent, \(S\) serves as a basis
for \(\spn{S}\). Thus, its dimension is \(\dim(\spn{S})=|S|\).
\end{pf}
\item \label{it:dim-subspace} Let \(W\) be a vector subspace of a vector space
\(V\). Then, \(\dim(W)\le \dim(V)\).

\begin{pf}
Let \(\beta_W\) and \(\beta_V\) be bases of \(W\) and \(V\) respectively. Then
\(\beta_W\) is a linearly independent subset of \(W\). Since \(W\subseteq V\),
\(\beta_W\) is also a linearly independent subset of \(V\). Hence, by
\Cref{prp:li-num-leq-dim}, we have \(|\beta_W|\le |\beta_V|\). Thus,
\(\dim(W)\le \dim(V)\).
\end{pf}
\end{enumerate}
\end{enumerate}
\subsection{Constructing Bases}
\label{subsect:construct-bases}
\begin{enumerate}
\item So far we have seen some examples of bases, but we do not yet have a
systematic method for obtaining bases. Thus we will introduce two important
approaches for constructing bases in \Cref{subsect:construct-bases}, namely
\emph{extension} and \emph{reduction}.

\item The intuitive idea behind \emph{extension} and \emph{reduction} is
related to \labelcref{it:span-set-not-smaller-than-basis} and
\labelcref{it:span-set-greater-than-basis-ld}:
\begin{itemize}
\item \emph{Extension:} From \labelcref{it:span-set-not-smaller-than-basis} we
know that if the cardinality of a subset \(S\) of \(V\) is less than
\(\dim(V)\), then \(\spn{S}\ne V\). The idea is thus as follows:
\begin{enumerate}
\item Start with a certain linearly independent subset \(S\) of \(V\) (which
may not span \(V\)).
\item  Keep adding vectors to \(S\) to form larger and larger \emph{linearly
independent} sets, until its cardinality reaches \(\dim(V)\)
\faIcon{arrow-right} ``extending'' the set \(S\).
\end{enumerate}
\item \emph{Reduction:} From \labelcref{it:span-set-greater-than-basis-ld} we
know that if the cardinality of a subset \(S\) of \(V\) is greater than
\(\dim(V)\), then \(S\) must not be linearly independent. The idea is thus as follows:
\begin{enumerate}
\item Start with a spanning set \(S\) of \(V\) (which may not be linearly
independent).
\item Keep removing vectors from \(S\) while ensuring it still spans \(V\),
until its cardinality reaches \(\dim(V)\) \faIcon{arrow-right} ``reducing'' the
set \(S\)
\end{enumerate}
\end{itemize}

\item To justify these two approaches mathematically, we shall utilize some
theorems below.

\begin{theorem}
\label{thm:add-vec-not-in-spn}
Let \(S\) be a linearly independent subset of a vector space \(V\), and let
\(\vect{v}\in V\). If \(\vect{v}\notin\spn{S}\), then \(S\cup\{\vect{v}\}\) is
also linearly independent.
\end{theorem}
\begin{pf}
The result is vacuously true if \(V=\{\vect{0}\}\), since the only linear
independent subset of \(V\) is \(\varnothing\), and
\(\vect{0}\in\spn{\varnothing}=\{\vect{0}\}\) always. Thus we never have
\(\vect{v}\notin\spn{S}\) in this case.

Now, assume that \(V\) is a nonzero vector space. Write
\(S=\{\vect{v}_1,\dotsc,\vect{v}_m\}\). Then, consider the equation
\begin{equation}
\label{eq:extend-li}
x_1\vect{v}_1+\dotsb+x_m\vect{v}_m+x_{m+1}\vect{v}=0.
\end{equation}
\underline{Case 1}: \(x_{m+1}\ne 0\).

In such case, we can rewrite \Cref{eq:extend-li} as
\[
\vect{v}=-\frac{x_1}{x_{m+1}}\vect{v}_1-\dotsb-\frac{x_m}{x_{m+1}}\vect{v}_m,
\]
meaning that \(\vect{v}\) is a linear combination of vectors in \(S\), thus
\(\vect{v}\in\spn{S}\), contradiction. So case 1 cannot happen.

\underline{Case 2}: \(x_{m+1}=0\).

In this case, we can simplify \Cref{eq:extend-li} as
\[
x_1\vect{v}_1+\dotsb+x_m\vect{v}_m=0,
\]
which implies \(x_1=\dotsb=x_m=0\) by the linear independence of \(S\).
Thus, the only solution to \Cref{eq:extend-li} is
\[
x_1=\dotsb=x_m=x_{m+1}=0,
\]
which means that \(S\cup\{\vect{v}\}\) is linearly independent.
\end{pf}
\item \Cref{thm:add-vec-not-in-spn} suggests how we can add vectors to a
linearly independent set while preserving its linear independence. But only
this is not enough for fully justifying the extension approach. We also need
the following result which guarantees that a basis must be obtained when the
cardinality of set reaches the dimension during the extension process.

\begin{theorem}
\label{thm:card-dim-li-basis}
Let \(m\) be the dimension of a vector space \(V\). If \(S\) is a linearly
independent subset of \(V\) with \(m\) vectors, then \(S\) is a basis for
\(V\).
\end{theorem}
\begin{pf}
Assume to the contrary that \(\spn{S}\ne V\). Then there exists \(\vect{v}\in
V\) such that \(\vect{v}\notin\spn{S}\). Next, using
\Cref{thm:add-vec-not-in-spn}, the set \(S\cup\{\vect{v}\}\) is linearly
independent. This contradicts \Cref{prp:li-num-leq-dim} since its cardinality is
\(|S\cup\{\vect{v}\}|=m+1>m=\dim(V)\)\footnote{When \(\vect{v}\notin\spn{S}\), it
implies in particular that \(\vect{v}\notin S\). Thus, \(|S\cup\{\vect{v}\}|\)
is indeed \(m+1\).}.
\end{pf}

\item Next, we consider theorems that justify \emph{reduction}. Likewise two
theorems are needed for fully justifying the reduction approach.

\begin{theorem}
\label{thm:remove-vec-still-spn}
Let \(S\) be a spanning set of a vector space \(V\). If \(S\) is linearly
\emph{dependent}, then there exists \(\vect{v}\in S\) such that
\(\spn{S\setminus \{\vect{v}\}}=\spn{S}=V\).
\end{theorem}
\begin{pf}
When \(V=\{\vect{0}\}\), this result again holds vacuously since the only
spanning set for \(V\) is the empty set \(\varnothing\), which must be linearly
independent.

Now assume that \(V\) is nonzero. Write \(S=\{\vect{v}_1,\dotsc,\vect{v}_m\}\).
Since \(S\) is linearly dependent, there exist scalars \(a_1,\dotsc,a_m\), not
all zero, such that
\[
a_1\vect{v}_1+\dotsb+a_m\vect{v}_m=0.
\]
Choose an \(i\in\{1,\dotsc,n\}\) with \(a_i\ne 0\). Then rewrite the equation as
\[
\vect{v}_i=-\frac{a_1}{a_i}\vect{v}_1-\dotsb-\frac{a_{i-1}}{a_i}\vect{v}_{i-1}
-\frac{a_{i+1}}{a_i}\vect{v}_{i+1}-\dotsb-\frac{a_m}{a_i}\vect{v}_m.
\]
Thus, \(\vect{v}_i\in\spn{S\setminus \{\vect{v}_i\}}\), and so
\[
\spn{S\setminus \{\vect{v}_i\}}=\spn{S}=V.
\]
\end{pf}

\begin{note}
The proof implicitly suggests that how we can find such vector \(\vect{v}\) to
be removed from the linearly dependent set \(S\). We just pick a vector in
\(S\) that is a linear combination of the other vectors in \(S\), which is
guaranteed to exist by \Cref{thm:remove-vec-still-spn}.
\end{note}
\item Next, we have a result that is analogous to \Cref{thm:card-dim-li-basis}.
But before proving it, we need the following lemma.
\begin{lemma}
\label{lma:basis-subset}
Let \(S\) be a finite subset of a vector space \(V\) with \(m\in\N_0\)
(distinct) nonzero vectors.  Then there exists a subset \(\beta\) of \(S\) such
that \(\beta\) is a basis for \(\spn{S}\).
\end{lemma}
\begin{pf}
We will prove by induction on the cardinality of \(S\): \(m\).

Firstly, the case \(m=0\) trivially holds since in such case we have
\(\spn{S}=\spn{\varnothing}=\{\vect{0}\}\), and thus we can just choose
\(\beta=S=\varnothing\) to be a basis for the zero vector space.

Assume for induction that the case \(m=k\) holds for a \(k\in\N_0\). Now,
consider any finite subset \(S\) of \(V\) with \(k+1\) nonzero vectors.

\underline{Case 1}: \(S\) is linearly independent.

Then we can simply choose \(\beta=S\) to be a basis for \(\spn{S}\).

\underline{Case 2}: \(S\) is linearly dependent.

Since \(S\) is a spanning set for \(\spn{S}\), by
\Cref{thm:remove-vec-still-spn}, there exists \(\vect{v}\in S\) such that
\(\spn{S\setminus \{\vect{v}\}}=\spn{S}\). Applying the induction hypothesis on
the set \(S\setminus \{\vect{v}\}\) (with \(k\) nonzero vectors), there exists
\(\beta\subseteq S\setminus \{\vect{v}\}\subseteq S\) such that \(\beta\) is a
basis for \(\spn{S\setminus \{\vect{v}\}}=\spn{S}\).

Thus, the case \(m=k+1\) holds, and so the result follows by induction.
\end{pf}

\item Now, we can prove the desired theorem.
\begin{theorem}
\label{thm:card-dim-spn-basis}
Let \(m\) be the dimension of a vector space \(V\). If \(S\) is a spanning set
of \(V\) with \(m\) vectors, then \(S\) is a basis for \(V\).
\end{theorem}
\begin{pf}
Assume to the contrary that \(S\) is not a basis for \(V\). Then by
\Cref{lma:basis-subset}, there exists a subset \(\beta\) of \(S\) such that
\(\beta\) is a basis for \(\spn{S}=V\). Since \(S\) is \emph{not} a basis for
\(V\), the subset inclusion must be proper and it must be the case that
\(|\beta|< |S|=m\). This implies that \(\dim(V)=|\beta|<m\), contradiction.
\end{pf}
\end{enumerate}
