\section{Vector Spaces}
\label{sect:vector-spaces}
\subsection{Vector Spaces and Vector Subspaces}
\begin{enumerate}
\item In \Cref{sect:vector-spaces}, we will start studying a main object of
interest in linear algebra: \emph{vector space}, which \emph{abstractizes} the
properties we know in \(\R^n\).
\item A \defn{vector space} over \(\R\) (or simply \emph{real} vector space) is
a nonempty set \(V\) on which two operations, called \defn{vector addition} and
\defn{scalar multiplication} are defined such that for any elements
\(\vect{u},\vect{v},\vect{w}\in V\) (called \defn{vectors} in \(V\)) and any
scalars \(a,b\in\R\), (i) the sum \(\vect{u}+\vect{v}\) and the scalar multiple
\(a\vect{v}\) are unique elements of \(V\), and (ii) the following axioms hold.
\begin{enumerate}[label={(\arabic*)}]
\item (commutativity) \(\vect{v}+\vect{u}=\vect{u}+\vect{v}\).
\item (associativity) \((\vect{u}+\vect{v})+\vect{w}=\vect{u}+(\vect{v}+\vect{w})\).
\item (zero vector) There exists an element \(\vect{0}\in V\) such that \(\vect{v}+\vect{0}=\vect{v}\).
\item (additive inverse) There exists an element \(\vc{-\vect{v}}\in V\) such that
\(\vect{v}+\vc{(-\vect{v})}=\vect{0}\).
\item \(1\vect{v}=\vect{v}\).
\item \((ab)\vect{v}=a(b\vect{v})\).
\item \(a(\vect{u}+\vect{v})=a\vect{u}+a\vect{v}\).
\item \((a+b)\vect{v}=a\vect{v}+b\vect{v}\).
\end{enumerate}
\begin{note}
In general, we can define a vector space over a \emph{field} \(K\), where the
scalars are elements of the \emph{field} \(K\). But here we shall focus on real
vector spaces. See MATH2102 for more details about this more general notion of
vector space.
\end{note}
\item With these axioms, we can deduce the following ``natural'' properties.
\begin{proposition}
\label{prp:vector-space-prop}
Let \(\vect{v}\) be a vector in a vector space \(V\). Then, the following hold.
\begin{enumerate}
\item The zero vector \(\vect{0}\) in axiom (3) is unique.
\item The scalar multiple \(0\vect{v}\) is the zero vector.
\item The additive inverse \(-\vect{v}\) in axiom (4) equals \((-1)\vect{v}\)
(and thus is uniquely determined by \(\vect{v}\)).
\item For any \(a\in\R\), we have \(a\vect{0}=\vect{0}\).
\end{enumerate}
\end{proposition}
\begin{pf}
\begin{enumerate}
\item Assume that there are two zero vectors \(\vect{0}\) and \(\vect{0}'\)
satisfying axiom (3). Then, by applying axioms (1) and (3), we have
\[
\vect{0}'\overset{\text{(3)}}{=}\vect{0}'+\vect{0}\overset{\text{(1)}}{=}
\vect{0}+\vect{0}\overset{\text{(3)}}{=}\vect{0}.
\]
\item Using the axioms (5) and (8), we have
\[
0\vect{v}+\vect{v}\overset{\text{(5)}}{=}0\vect{v}+1\vect{v}
\overset{\text{(8)}}{=}(0+1)\vect{v}
=1\vect{v}
\overset{\text{(5)}}{=}\vect{v}.
\]
Adding additive inverse \(-\vect{v}\) on both sides, we get
\[
(0\vect{v}+\vect{v})+(-\vect{v})=\vect{v}+(-\vect{v})
\overset{\text{(2), (4)}}{\implies}
0\vect{v}+\vect{0}=\vect{0}
\overset{\text{(3)}}{\implies}
0\vect{v}=\vect{0}.
\]
\item Consider:
\begin{align*}
\vect{v}+(-\vect{v})&=\vect{0}&\text{(4)}\\
\implies(-1)\vect{v}+[\vect{v}+(-\vect{v})]&=(-1)\vect{v}+\vect{0}\\
\implies[(-1)\vect{v}+\vect{v}]+(-\vect{v})&=(-1)\vect{v}&\text{(2), (3)}\\
\implies[(-1)\vect{v}+1\vect{v}]+(-\vect{v})&=(-1)\vect{v}&\text{(5)}\\
\implies 0\vect{v}+(-\vect{v})&=(-1)\vect{v}&\text{(8)}\\
\implies -\vect{v}&=(-1)\vect{v}&\text{(b), (1), (3)}.
\end{align*}
\item Setting \(\vect{v}=\vect{0}\) in (b), we have \(0\vect{0}=\vect{0}\).
Hence,
\[
a\vect{0}=a(0\vect{0})=(a0)\vect{0}=0\vect{0}=\vect{0}.
\]
\end{enumerate}
\end{pf}
\item Some examples of vector space (the vector addition and scalar
multiplication are naturally defined in the usual way for the following):
\begin{enumerate}
\item The set of \(\R^n\) of column vectors.
\item The set of \(m\times n\) matrices, denoted by \(M_{m\times n}(\R)\).
\begin{note}
The zero vector in this vector space is the \emph{zero matrix} \(O_{m\times n}\).
\end{note}
\item The set of all functions from \(\R\) to \(\R\).
\begin{note}
The zero vector in this vector space is the constant zero function, i.e.,
function \(f:\R \to\R\) defined by \(f(x)=0\) for any \(x\in\R\).
\end{note}
\end{enumerate}
\item Just like a set can have \emph{subsets}, a vector space can have (vector)
\emph{subspaces}. The idea is that we want to extract a ``part'' of a vector
space such that the part is still a valid vector space on its own.

\item Let \(V\) be a vector space. A \emph{nonempty} subset \(W\) of \(V\) is a
\defn{vector subspace} (or simply \defn{subspace}) of \(V\) if for any
\(\vect{u},\vect{v}\in W\) and any scalar \(a\in\R\),
\begin{enumerate}
\item (closed under addition) \(\vect{u}+\vect{v}\in W\).
\item (closed under scalar multiplication) \(a\vect{v}\in W\).
\end{enumerate}
\begin{warning}
Do not forget the condition that the set needs to be nonempty! In practice,
this can be verified conveniently by showing that \(\vect{0}\in W\).
\end{warning}
\item The following property is useful for showing that a given set is
\emph{not} a vector subspace.
\begin{proposition}
\label{prp:vec-subsp-contain-zero}
Let \(W\) be a vector subspace of a vector space \(V\). Then, the zero vector
\(\vect{0}\) in \(V\) belongs also to \(W\).
\end{proposition}
\begin{pf}
Consider any \(\vect{v}\in W\subseteq V\), which exists since \(W\) is
nonempty. Then note that
\[
\vect{0}=-\vect{v}+\vect{v}=(-1)\vect{v}+\vect{v}\in W
\]
where the last ``\(\in\)'' follows from the closedness under addition and
scalar multiplication.
\end{pf}

From this result, we know that if a subset \(W\) of a vector space \(V\) does
\emph{not} contain the zero vector \(\vect{0}\) in \(V\), then it cannot
possibly be a vector subspace.

\item The following result justifies that a vector subspace is indeed a vector
space.
\begin{proposition}
\label{prp:vec-subsp-is-vec-sp}
Let \(W\) be a vector subspace of a vector space \(V\). Then \(W\) is also a
vector space, with the vector addition and scalar multiplication from \(V\).
\end{proposition}
\begin{pf}
Firstly, closedness under addition and scalar multiplication guarantees that
vector addition and scalar multiplication must yield unique elements in \(W\).
So it remains to check the axioms of vector space. In fact, we only need to
check axioms (3) and (4) since other axioms must hold due to the fact that
\(W\subseteq V\) and \(V\) is a vector space.

Axiom (3) follows from \Cref{prp:vec-subsp-contain-zero}. Axiom (4) holds by
noting that, for any \(\vect{v}\in W\), there exists \((-1)\vect{v}\in W\) such
that
\[
\vect{v}+(-1)\vect{v}=(1-1)\vect{v}=0\vect{v}=\vect{0}.
\]
\end{pf}
\item Some examples of vector subspaces:
\begin{enumerate}
\item A vector space \(V\) is always a vector subspace of itself.
\item Given any vector space \(V\), the subset of \(V\) containing only the
zero vector (\(\{\vect{0}\}\)) is a vector subspace of \(V\). It is
called the \defn{zero subspace} of \(V\).
\item The set of all diagonal \(n\times n\) matrices is a vector subspace of the
vector space \(M_{n\times n}(\R)\).
\item The set of all real polynomials is a vector subspace of the vector
space of all functions from \(\R\) to \(\R\).
\end{enumerate}
\end{enumerate}
\subsection{Spanning Sets}
\begin{enumerate}
\item Consider a fixed vector \(\vect{v}\in\R^n\). Note that the set
\(\{c\vect{v}:c\in\R\}\) is a vector subspace of \(\R^n\). Here, we use a
single vector to ``yield''  a vector subspace.

Next, consider two vectors \(\vect{i}\) and \(\vect{j}\) in \(\R^2\). Note that
the set \(\{a\vect{i}+b\vect{j}:a,b\in\R\}\) is indeed just \(\R^2\). So it
turns out that the whole vector space \(\R^2\) can be obtained by using just
two vectors. They may be seen as ``cores'' of \(\R^2\).

\item We have used the notion of \emph{linear combination} above, which is
defined in \labelcref{it:lin-comb} in the context of vector space \(\R^n\).
Here, we will define it more generally as follows.

Let \(S\) be a nonempty subset of \(V\). A vector \(\vect{v}\in V\) is a
\defn{linear combination of vectors in \(S\)} if there exist a \emph{finite}
number of vectors \(\vect{v}_1,\dotsc,\vect{v}_{n}\in S\) and scalars
\(a_1,\dotsc,a_n\in\R\) such that
\[
\vect{v}=a_1\vect{v}_1+\dotsb+a_n\vect{v}_n.
\]
In such case, we call \(\vect{v}\) as a \defn{linear combination of vectors
\(\vect{v}_1,\dotsc,\vect{v}_n\)}.

\item Some examples of linear combination:
\begin{enumerate}
\item In \(\R^3\), \(\mqty[1\\2\\3]\) is a linear combination of
\(\mqty[1\\0\\0]\), \(\mqty[0\\1\\0]\), and \(\mqty[0\\0\\1]\).
\item In \(M_{2\times 2}(\R)\), \(\mqty[1&3\\ 2&0]\) is a linear combination of
\(\mqty[1&3\\ 0&0]\), \(\mqty[0&0\\ 2&-1]\), and \(\mqty[0&0\\ 2&1]\) since
\[
\mqty[1&3\\ 2&0]=\mqty[1&3\\ 0&0]+\frac{1}{2}\mqty[0&0\\ 2&-1]+\frac{1}{2}\mqty[0&0\\ 2&1].
\]
\item In the vector space of all polynomials, \(1+2x-6x^5\) is a linear
combination of \(1+3x\) and \(x+6x^5\) since
\[
1+2x-6x^5=(1+3x)-(x+6x^5).
\]
\end{enumerate}
\item Let \(S\) be a nonempty subset of a vector space \(V\). The \defn{span}
of \(S\), denoted by \(\spn{S}\), is the set of all linear
combinations of the vectors in \(S\), i.e.,
\[
\spn{S}=\qty{\sum_{i=1}^{k}a_i\vect{v}_i:a_1,\dotsc,a_k\in\R, \vect{v}_1,\dotsc,\vect{v}_k\in S, k\in\N}
\]
Conventionally, we define \(\spn{\varnothing}=\{\vect{0}\}\). \begin{intuition}
The span of empty set is like a set containing an ``empty sum'', which should
naturally be the zero vector \(\vect{0}\) (representing ``nothing'').
\end{intuition}

\item It turns out that spanning always results in a vector subspace.
\begin{proposition}
\label{prp:span-vec-subsp}
Let \(V\) be a vector space and \(S\) be any subset of \(V\). Then, \(\spn{S}\)
is a vector subspace of \(V\).
\end{proposition}
\begin{pf}
If \(S=\varnothing\), then there is nothing to prove as
\(\spn{S}=\{\vect{0}\}\) is simply the zero subspace in this case.
Henceforth assume that \(S\ne\varnothing\). Fix any
\(\vect{u},\vect{v}\in\spn{S}\). Then we can write
\[
\vect{u}=a_1\vect{u}_1+\dotsb+a_r\vect{u}_r,
\]
for some \(a_1,\dotsc,a_r\in\R\) and \(\vect{u}_1,\dotsc,\vect{u}_r\in S\).
Also, we can write
\[
\vect{v}=b_1\vect{v}_1+\dotsb+b_s\vect{v}_s,
\]
for some \(b_1,\dotsc,b_s\in\R\) and \(\vect{v}_1,\dotsc,\vect{v}_s\in S\).

Now, it remains to check that \(\vect{u}+\vect{v}\in\spn{S}\) and
\(c\vect{v}\in\spn{S}\) for any \(c\in\R\).

\underline{\(\vect{u}+\vect{v}\in\spn{S}\)}: Note that
\[
\vect{u}+\vect{v}
=(a_1\vect{u}_1+\dotsb+a_r\vect{u}_r)+(b_1\vect{v}_1+\dotsb+b_s\vect{v}_s)
\]
is a linear combination of vectors
\(\vect{u}_1,\dotsc,\vect{u}_r,\vect{v}_1,\dotsc,\vect{v}_s\) in \(S\). Thus,
\(\vect{u}+\vect{v}\in\spn{S}\).

\underline{\(c\vect{v}\in\spn{S}\) for any \(c\in\R\)}: Note that for any
\(c\in\R\),
\[
c\vect{v}=c(b_1\vect{v}_1+\dotsb+b_s\vect{v}_s)
=(cb_1)\vect{v}_1+\dotsb+(cb_s)\vect{v}_s
\]
is a linear combination of vectors in \(S\), hence \(c\vect{v}\in\spn{S}\).
\end{pf}
\item A special kind of vector subspace obtained by spanning is the
\emph{column space}, which can be got by spanning the \emph{columns} of a
matrix.

Let \(A\) be an \(m\times n\) matrix and let \(\vect{v}_1,\dotsc,\vect{v}_n\)
be the columns of \(A\). Then,
\[
\col{A}=\spn{\{\vect{v}_1,\dotsc,\vect{v}_n\}}
\]
is called the \defn{column space} of \(A\).

\item\label{it:col-space-expr} By definition of matrix-vector product, we can
show that
\[
\col{A}=\{A\vect{x}\in\R^n:\vect{x}\in\R^m\}.
\]
\begin{pf}
``\(\subseteq\)'': Fix any \(\vect{u}\in\col{A}\). Then, we have
\[
\vect{u}=x_1\vect{v}_1+\dotsb+x_n\vect{v}_n
\]
for some scalars \(x_1,\dotsc,x_n\in\R\). But then we can just express it as
\[
\vect{u}=A\vect{x}
\]
where \(\vect{x}=\mqty[x_1&\cdots& x_n]^{T}\in\R^m\) by definition of matrix-vector product.

``\(\supseteq\)'': Fix any \(\vect{u}\in\{A\vect{x}\in\R^n:\vect{x}\in\R^m\}\).
Then, \(\vect{u}=A\vect{x}\) for some \(\vect{x}\in\R^m\). Then, applying the
definition of matrix-vector product again, writing
\(\vect{x}=\mqty[x_1&\cdots &x_n]^{T}\), we have
\[
\vect{u}=x_1\vect{v}_1+\dotsb+x_n\vect{v}_n,
\]
which means that \(\vect{u}\in\spn{\{\vect{v}_1,\dotsc,\vect{v}_n\}}\).
\end{pf}
\item A subset \(S\) of a vector space \(V\) is said to \defn{span} \(V\), or
is a \defn{spanning set} of \(V\), if \(\spn{S}=V\).

Since every linear combination of vectors in \(S\subseteq V\) belongs to \(V\)
by the definition of vector space, we always have \(\spn{S}\subseteq V\).
Hence, to show that \(S\) spans \(V\), it actually suffices to show that
\(\spn{S}\supseteq V\), i.e., \emph{every vector in \(V\) is a linear
combination of vectors in \(S\).}

\item It turns that the concept of \emph{solving system of linear equations} we
have studied in \Cref{sect:sle} is useful for showing this.

For example, consider \(V=\R^m\) and
\(S=\{\vect{v}_1,\dotsc,\vect{v}_n\}\subseteq V\). Then, for any
\(\vect{u}=\mqty[u_1&\cdots&u_m]^{T}\in\R^m\), we need to investigate whether
the following equation, with unknowns \(x_1,\dotsc,x_n\), has any solution:
\[
x_1\vect{v}_1+\dotsb+x_n\vect{v}_n=\vect{u}.
\]
Writing \(\vect{v}_j=\mqty[v_{1j}\\\vdots\\v_{mj}]\) for any \(j=1,\dotsc,n\),
we can write the equation above more explicitly as a system of \(m\) linear
equations in \(n\) unknowns:
\[
\begin{cases}
v_{11}x_1+\dotsb+v_{1n}x_n=u_1,\\
v_{21}x_1+\dotsb+v_{2n}x_n=u_2,\\
\qquad\vdots\\
v_{m1}x_1+\dotsb+v_{mn}x_n=u_m.\\
\end{cases}
\]
Its augmented matrix is
\[
\left[\begin{array}{@{}cccc|c@{}}
v_{11}&v_{12}&\cdots&v_{1n}&u_{1}\\
v_{21}&v_{22}&\cdots&v_{2n}&u_{2}\\
\vdots&\vdots&\ddots&\vdots&\vdots\\
v_{m1}&v_{m2}&\cdots&v_{mn}&u_{m}\\
\end{array}\right].
\]
Then, we know that \(S\) spans \(V\) iff the system is \emph{consistent}. So
methods for determining consistency in \Cref{subsect:sys-sol-num} can be
utilized to determine whether \(S\) spans \(V\) or not.
\end{enumerate}
\subsection{Linear Independence}
\begin{enumerate}
\item While \emph{spanning set} is about the \emph{existence} of solution to a
system, \emph{linear independence} is about the \emph{uniqueness} of solution
to a system.

\item A subset \(S\) of a vector space \(V\) is called \defn{linearly
dependent} if there exist distinct vectors \(\vect{v}_1,\dotsc,\vect{v}_n\in
S\), and scalars \(a_1,\dotsc,a_n\in\R\) which are \emph{not all zero} such that
\[
a_1\vect{v}_1+\dotsb+a_n\vect{v}_n=\vect{0},
\]
for some \(n\in\N\).
\begin{intuition}
This means that we can express a vector in \(S\) as a linear combination of
some other vectors in the same set \(S\), so some vectors in \(S\) are
``related linearly'' \faIcon{arrow-right} ``linearly dependent''.

The equality would hold if the scalars were all zero, \emph{regardless} of what
\(S\) is. So we would like to exclude this ``boring'' case.
\end{intuition}

\item A subset \(S\) of a vector space \(V\) is called \defn{linearly
independent} if \(S\) is \emph{not} linearly dependent, i.e., for any
\(n\in\N\), the only scalars \(a_1,\dotsc,a_n\in\R\) satisfying
\[
a_1\vect{v}_1+\dotsb+a_n\vect{v}_n=\vect{0},\quad\text{where
\(\vect{v}_1,\dotsc,\vect{v}_n\in S\) are distinct},
\]
are the trivial one: \(a_1=\dotsb=a_n=0\), i.e., symbolically,
\[
a_1\vect{v}_1+\dotsb+a_n\vect{v}_n=\vect{0}
\implies a_1=\dotsb=a_n=0.
\]
In other words, for any finitely
many distinct \(\vect{v}_1,\dotsc,\vect{v}_n\in S\), the homogeneous system
(with unknowns \(a_1,\dotsc,a_n\))
\[
\mqty[\vect{v}_1&\cdots&\vect{v}_n]\mqty[a_1\\\vdots\\ a_n]=\mqty[0\\\vdots\\ 0]
\]
only has the trivial solution \(a_1=\dotsb=a_n=0\). Since a homogeneous system
always has the trivial solution, this is equivalent to saying that the
homogeneous system has a \emph{unique} solution. So again the methods discussed
in \Cref{subsect:sys-sol-num} are useful for determining linear independence.

\begin{note}
If we have \(S=\{\vect{v}_1,\dotsc,\vect{v}_n\}\), sometimes we say
``\(\vect{v}_1,\dotsc,\vect{v}_n\) are linearly (in)dependent'' to mean
``\(S\) is linearly (in)dependent''.
\end{note}

\item \label{it:two-nonzero-vec-lin-dep}
We can have a more ``intuitive'' equivalent definition for linear
dependence when we focus on a set of two nonzero vectors.

Consider a subset \(S=\{\vect{u},\vect{v}\}\) of a vector space \(V\), where
\(\vect{u}\) and \(\vect{v}\) are nonzero. Then \(S\) is linearly dependent iff
\(\vect{u}=a\vect{v}\) for some \(a\ne 0\).

\begin{pf}
``\(\Leftarrow\)'': Assume that \(\vect{u}=a\vect{v}\) for some \(a\ne 0\).
Then, we can write
\[
(1)\vect{u}+(-a)\vect{v}=\vect{0},
\]
so \(S\) is linearly dependent.

``\(\Rightarrow\)'': Assume that \(S\) is linearly dependent. Then there exist
scalars \(a_1,a_2\in\R\), not all zero, such that
\[
a_1\vect{u}+a_2\vect{v}=\vect{0}.
\]
We claim that \(a_1\ne 0\). To prove this, assume to the contrary that
\(a_1=0\). Then, we have \(a_2\ne 0\) and also \(a_2\vect{v}=\vect{0}\). This
implies \(\vect{v}=\vect{0}\), contradiction. Similarly, we can show that
\(a_2\ne 0\).

Hence, we can write
\[
\vect{u}=\underbrace{\frac{-a_2}{a_1}}_{\ne 0}\vect{v}
\]
as desired.
\end{pf}
\item \label{it:lin-dep-subsets}
Let \(V\) be a vector space and let \(S_1\subseteq S_2\subseteq V\). Then, the
following hold.
\begin{enumerate}
\item If \(S_2\) is linearly independent, then \(S_1\) is also linearly independent.
\item If \(S_1\) is linearly dependent, then \(S_2\) is also linearly
dependent.
\end{enumerate}
\begin{pf}
\begin{enumerate}
\item Assume that \(S_2\) is linearly independent. Then for any \(n\in\N\) and
any distinct vectors \(\vect{v}_1,\dotsc,\vect{v}_n\in S_2\), we have
\[
a_1\vect{v}_1+\dotsb+a_n\vect{v}_n=\vect{0}
\implies a_1=\dotsb=a_n=0.
\]
But as \(S_1\subseteq S_2\), this implication must also hold for arbitrary
distinct vectors in \(S_1\).

\item The statement is just the contrapositive of (a), and hence is true also.
\end{enumerate}
\end{pf}

However, the converse does \emph{not} hold in general. WLOG, we focus on the
converse of (a): ``If \(S_1\) is linearly independent, then \(S_2\) is linearly
independent.'' Take any \(S_1\subseteq V\) which is linearly independent and
take \(S_2=S_1\cup\{\vect{0}\}\).  Then, \(S_2\supseteq S_1\) but \(S_2\) is
always linearly dependent.
\end{enumerate}
\subsection{Bases}
\begin{enumerate}
\item The next concept to be discussed is \emph{basis}, which neatly connects
the concepts of \emph{spanning set} and \emph{linear independence}.  A
\defn{basis} for a vector space \(V\) is a \emph{linearly independent} subset
of \(V\) which also \emph{spans} \(V\).

\begin{intuition}
A basis can be seen as a ``minimal'' spanning set for \(V\), without
``redundancy''.
\end{intuition}

\item To illustrate why a basis can be seen as a ``minimal'' spanning set for
\(V\), consider the following results. Let \(\beta\) be a basis for a vector
space \(V\).
\begin{enumerate}
\item \label{it:span-set-not-smaller-than-basis} If \(S\) is a proper subset of \(\beta\), then \(\spn{S}\ne V\).

\begin{pf}
Assume that \(S\) is a proper subset of \(\beta\). Then, there exists
\(\vect{v}\in\beta\subseteq V\) with \(\vect{v}\notin S\). Now, suppose on the
contrary that \(\vect{v}\in\spn{S}\). Then, we can write
\[
\vect{v}=a_1\vect{v}_1+\dotsb+a_n\vect{v}_n
\]
where \(a_1,\dotsc,a_n\in\R\), \(v_1,\dotsc,v_n\in S\), and \(n\in\N\).
This implies that
\[
a_1\vect{v}_1+\dotsb+a_n\vect{v}_n+(-1)\vect{v}=\vect{0},
\]
thus \(\{\vect{v}_1,\dotsc,\vect{v}_n,\vect{v}\}\subseteq \beta\) is \emph{not}
linearly independent, which would imply that \(\beta\) is \emph{not} linearly
independent also by \labelcref{it:lin-dep-subsets}, contradiction. This means
\(\vect{v}\notin\spn{S}\), so we have found an element in \(V\) \emph{not} in
\(\spn{S}\), meaning that \(\spn{S}\ne V\).
\end{pf}

\begin{intuition}
This means that there is not ``smaller'' spanning set for \(V\) than the
``minimal'' one.
\end{intuition}
\item \label{it:span-set-greater-than-basis-ld} If \(\beta\) is a proper subset of a set \(S\subseteq V\), then \(S\) is
not linearly independent.

\begin{pf}
Assume that \(\beta\) is a proper subset of \(S\subseteq V\). Then there exists
\(\vect{v}\in S\subseteq V\) with \(\vect{v}\notin \beta\). Since
\(\beta=\{\vect{b}_1,\dotsc,\vect{b}_n\}\) spans \(V\), we can write
\[
\vect{v}=a_1\vect{b}_1+\dotsb+a_n\vect{b}_n
\]
for some scalars \(a_1,\dotsc,a_n\in\R\). But since \(\beta\) is a proper
subset of \(S\), we have \(S\supseteq
\{\vect{b}_1,\dotsc,\vect{b}_n,\vect{v}\}\). By writing the equation above as
\[
a_1\vect{b}_1+\dotsb+a_n\vect{b}_n+(-1)\vect{v}=\vect{0},
\]
we note that the latter set \(\{\vect{b}_1,\dotsc,\vect{b}_n,\vect{v}\}\) is
\emph{not} linearly independent, so do \(S\) (by \labelcref{it:lin-dep-subsets}).
\end{pf}
\begin{intuition}
Note that \(S\) also spans \(V\) since \(\beta\) is a proper subset of \(S\).
However, as a set ``larger'' than the ``minimal'' spanning set, it would
contain some ``redundancy'' \faIcon{arrow-right} not linearly independent.
\end{intuition}
\end{enumerate}
\item Consider the vector space \(\R^n\). A standard example of basis of
\(\R^n\) is the set \(\beta=\{\vect{e}_1,\dotsc,\vect{e}_n\}\), where the
elements are the standard vectors. It is called the \defn{standard basis} for
\(\R^n\).

\begin{pf}
We can prove that the standard basis is indeed a basis for \(\R^n\).

Firstly, we show that it spans \(\R^n\). For any
\(\vect{v}=\mqty[v_1&\cdots&v_n]^{T}\in\R^n\), we can write
\[
\vect{v}=v_1\vect{e}_1+\dotsb+v_n\vect{e}_n,
\]
so \(\vect{v}\in\spn{\beta}\). This implies that \(\spn{\beta}=\R^n\).

Next, to show the linear independence of \(\beta\), consider first the equation
\[
x_1\vect{e}_1+\dotsb+x_n\vect{e}_n=\vect{0}.
\]
It can be rewritten as a homogeneous system
\[
I_n\mqty[x_1\\ \vdots\\ x_n]=\mqty[0\\ \vdots\\ 0],
\]
which clearly has the unique solution \(x_1=\dotsb=x_n=0\).
\end{pf}
\item Note that basis is \emph{not unique}. For example, for the vector space
\(\R^2\), two possible bases are the standard basis \(\{\vect{i},\vect{j}\}\),
and the set \(\{2\vect{i},2\vect{j}\}\).

\item Let \(V\) be a vector space and \(S\) be any linearly independent subset
of \(V\). Then, \(S\) is a basis for its span \(\spn{S}\).

\begin{pf}
Linear independence of \(S\) follows from assumption, and by definition \(S\)
must be a spanning set of \(\spn{S}\), where \(\spn{S}\) is considered as a
vector space.
\end{pf}

As a special case, if \(V=\{\vect{0}\}\), then the only linearly
independent subset of \(V\) is \(\varnothing\), so the empty set
\(\beta=\varnothing\) is a basis for \(\spn{\varnothing}=\{\vect{0}\}=V\).

\item The main result about \emph{basis} is the following.
\begin{theorem}
\label{thm:basis-unique-lin-comb}
Let \(V\) be a nonzero vector space and \(\beta\) be a subset of \(V\). Then,
\(\beta\) is a basis for \(V\) iff for any vector \(\vect{u}\in V\),
\(\vect{u}\) can be uniquely written as a linear combination of vectors in
\(\beta\), i.e.,
\[
\vect{u}=a_1\vect{v}_1+\dotsb+a_n\vect{v}_n
\]
for some \emph{unique} scalars \(a_1,\dotsc,a_n\in\R\), with
\(\beta=\{\vect{v}_1,\dotsc,\vect{v}_n\}\).
\end{theorem}
\begin{intuition}
The spanning property of basis corresponds to the \emph{existence} of such
linear combination, and the linear independence of basis corresponds to the
\emph{uniqueness} of such linear combination.
\end{intuition}

\begin{pf}
``\(\Rightarrow\)'': Assume that \(\beta\) is a basis for \(V\), and fix any
vector \(\vect{u}\in V\).

\underline{Existence}: Since \(\beta\) spans \(V\), there exist scalars
\(a_1,\dotsc,a_n\in\R\) such that
\[
\vect{u}=a_1\vect{v}_1+\dotsb+a_n\vect{v}_n.
\]

\underline{Uniqueness}: Suppose that we can also write
\(\vect{u}=b_1\vect{v}_1+\dotsb+b_n\vect{v}_n\) for some scalars
\(b_1,\dotsc,b_n\). Then, we have
\[
\vect{u}=a_1\vect{v}_1+\dotsb+a_n\vect{v}_n=b_1\vect{v}_1+\dotsb+b_n\vect{v}_n,
\]
which implies
\[
(a_1-b_1)\vect{v}_1+\dotsb+(a_n-b_n)\vect{v}_n=\vect{0}.
\]
By linear independence of \(\beta\), it follows that
\[
a_1-b_1=\dotsb=a_n-b_n=0,
\]
which means \(a_1=b_1,\dotsc,a_n=b_n\).

``\(\Leftarrow\)'': Assume that for any \(\vect{u}\in V\), we can write
\[
\vect{u}=a_1\vect{v}_1+\dotsb+a_n\vect{v}_n
\]
for some \emph{unique} scalars \(a_1,\dotsc,a_n\in\R\), with
\(\beta=\{\vect{v}_1,\dotsc,\vect{v}_n\}\). This immediately shows that
\(\spn{\beta}=V\). So it remains to prove that \(\beta\) is linearly
independent. Consider the equation
\[
c_1\vect{v}_1+\dotsb+c_n\vect{v}_n=\vect{0}=0\vect{v}_1+\dotsb+0\vect{v}_n.
\]
By the uniqueness, we must have \(c_1=\dotsb=c_n=0\), as desired.
\end{pf}
\end{enumerate}
\subsection{Dimensions}
\label{subsect:dim}
\begin{enumerate}
\item From now on, we shall focus on vector spaces with \emph{finite} bases
unless otherwise specified. Vector spaces with \emph{infinite} bases are
studied in \emph{functional analysis}, which is a much more advanced topic.
(See MATH4404 for more details.)

\item In \Cref{subsect:dim}, we shall discuss a ``famous'' concept:
\emph{dimension}. Most of us should have some intuitive idea about it, but here
we will try to formalize this notion mathematically (in the context of linear
algebra).

\item Before discussing the concept of dimension, we shall first consider
several preliminary results.

\begin{proposition}
\label{prp:li-num-leq-dim}
Let \(\beta\) be a basis for a vector space \(V\). If \(\beta'\) is a
linearly independent subset of \(V\), then \(|\beta'|\le |\beta|\). (Here
\(|\cdot|\) denotes cardinality.)
\end{proposition}
\begin{pf}
Firstly, if \(V\) is the zero vector space \(\{\vect{0}\}\), this result is
trivial since the only linearly independent subset of \(V\) is the empty set
\(\varnothing\). Thus, henceforth we suppose that \(V\) is a nonzero vector
space.

Let \(m=|\beta|\) and \(n=|\beta'|\). Then, we write
\(\beta=\{\vect{v}_1,\dotsc,\vect{v}_m\}\) and
\(\beta'=\{\vect{u}_1,\dotsc,\vect{u}_n\}\).

Since \(\beta\) spans \(V\) and \(\beta'\) is a subset of \(V\), for any
\(j=1,\dotsc,n\), we can write
\[
\vect{u}_j=a_{1j}\vect{v}_1+\dotsb+a_{mj}\vect{v}_m
\]
for some scalars \(a_{1j},\dotsc,a_{mj}\).

Now, we set
\[
x_1\vect{u}_1+\dotsb+x_n\vect{u}_n=0,
\]
which can be rewritten as
\[
\sum_{i=1}^{m}(a_{i1}x_1+\dotsb+a_{in}x_n)\vect{v}_i=0.
\]
By linear independence of \(\beta=\{\vect{v}_1,\dotsc,\vect{v}_m\}\), we
obtain the following homogeneous system of linear equations
\[
\begin{cases}
a_{11}x_{1}+\dotsb+a_{1n}x_{n}=0\\
a_{21}x_{1}+\dotsb+a_{2n}x_{n}=0\\
\hspace{2cm}\vdots\\
a_{m1}x_1+\dotsb+a_{mn}x_n=0.
\end{cases}
\]
Now, assume to the contrary that \(n>m\). In such case, by
\Cref{prp:homo-var-more-than-eqs-inf}, this system would have a non-trivial
solution, contradicting to the linear independence of
\(\beta'=\{\vect{u}_1,\dotsc,\vect{u}_n\}\).
\end{pf}

\item
Using \Cref{prp:li-num-leq-dim}, we can show the following relationship between
any two bases for a vector space \(V\).

\begin{corollary}
\label{cor:two-bases-same-num}
Let \(\beta_1\) and \(\beta_2\) be any two bases of a vector space \(V\). Then,
\(\beta_1\) and \(\beta_2\) contain the same number of vectors.
\end{corollary}

\begin{pf}
Setting \(\beta=\beta_1\) and \(\beta'=\beta_2\) in \Cref{prp:li-num-leq-dim}
gives \(|\beta_2|\le|\beta_1|\). On the other hand, setting \(\beta=\beta_2\)
and \(\beta'=\beta_1\) in \Cref{prp:li-num-leq-dim} gives
\(|\beta_1|\le|\beta_2|\). Hence, we have \(|\beta_1|=|\beta_2|\).
\end{pf}

\item From \Cref{cor:two-bases-same-num}, we see that the \emph{cardinality of
basis} is independent from the choice of basis. It only depends on the
underlying vector space. This suggests the notion of \emph{dimension}. The
number of vectors in a basis for a vector space \(V\) is called the
\defn{dimension} of \(V\), denoted by \(\dim(V)\).

\begin{note}
Roughly speaking, dimension may be seen as the number of ``degrees of
freedom'' or ``independent parameters''.
\end{note}

Examples:
\begin{itemize}
\item The dimension of \(\R^n\) is \(n\) (as expected) since the
standard basis \(\{\vect{e}_1,\dotsc,\vect{e}_n\}\) of \(\R^n\) has
\(n\) vectors.
\item The dimension of the zero vector space \(V=\{\vect{0}\}\) is 0
since a basis (in fact, the only basis) for \(V\) is \(\varnothing\).
Indeed, the zero vector space is the only vector space with zero
dimension.
\end{itemize}

\item To close \Cref{subsect:dim}, we give some results regarding dimension.
\begin{enumerate}
\item \label{it:dim-span} Let \(S\) be a finite linearly independent subset of
a vector space \(V\). Then, \(\dim(\spn{S})=|S|\).

\begin{pf}
Since \(S\) is a spanning set of \(\spn{S}\) (where \(\spn{S}\) is considered
as a vector space) and \(S\) is linearly independent, \(S\) serves as a basis
for \(\spn{S}\). Thus, its dimension is \(\dim(\spn{S})=|S|\).
\end{pf}
\item \label{it:dim-subspace} Let \(W\) be a vector subspace of a vector space
\(V\). Then, \(\dim(W)\le \dim(V)\).

\begin{pf}
Let \(\beta_W\) and \(\beta_V\) be bases of \(W\) and \(V\) respectively. Then
\(\beta_W\) is a linearly independent subset of \(W\). Since \(W\subseteq V\),
\(\beta_W\) is also a linearly independent subset of \(V\). Hence, by
\Cref{prp:li-num-leq-dim}, we have \(|\beta_W|\le |\beta_V|\). Thus,
\(\dim(W)\le \dim(V)\).
\end{pf}
\end{enumerate}
\end{enumerate}
\subsection{Constructing Bases: Extension and Reduction}
\label{subsect:construct-bases}
\begin{enumerate}
\item So far we have seen some examples of bases, but we do not yet have a
systematic method for obtaining bases. Thus we will introduce two important
approaches for constructing bases in \Cref{subsect:construct-bases}, namely
\emph{extension} and \emph{reduction}.

\item The intuitive idea behind \emph{extension} and \emph{reduction} is
related to \labelcref{it:span-set-not-smaller-than-basis} and
\labelcref{it:span-set-greater-than-basis-ld}:
\begin{itemize}
\item \emph{Extension:} From \labelcref{it:span-set-not-smaller-than-basis} we
know that if the cardinality of a subset \(S\) of \(V\) is less than
\(\dim(V)\), then \(\spn{S}\ne V\). The idea is thus as follows:
\begin{enumerate}
\item Start with a certain linearly independent subset \(S\) of \(V\) (which
may not span \(V\)).
\item  Keep adding vectors to \(S\) to form larger and larger \emph{linearly
independent} sets, until its cardinality reaches \(\dim(V)\)
\faIcon{arrow-right} ``extending'' the set \(S\).
\end{enumerate}
\item \emph{Reduction:} From \labelcref{it:span-set-greater-than-basis-ld} we
know that if the cardinality of a subset \(S\) of \(V\) is greater than
\(\dim(V)\), then \(S\) must not be linearly independent. The idea is thus as follows:
\begin{enumerate}
\item Start with a spanning set \(S\) of \(V\) (which may not be linearly
independent).
\item Keep removing vectors from \(S\) while ensuring it still spans \(V\),
until its cardinality reaches \(\dim(V)\) \faIcon{arrow-right} ``reducing'' the
set \(S\)
\end{enumerate}
\end{itemize}

\item To justify these two approaches mathematically, we shall utilize some
theorems below.

\begin{theorem}
\label{thm:add-vec-not-in-spn}
Let \(S\) be a linearly independent subset of a vector space \(V\), and let
\(\vect{v}\in V\). If \(\vect{v}\notin\spn{S}\), then \(S\cup\{\vect{v}\}\) is
also linearly independent.
\end{theorem}
\begin{pf}
The result is vacuously true if \(V=\{\vect{0}\}\), since the only linear
independent subset of \(V\) is \(\varnothing\), and
\(\vect{0}\in\spn{\varnothing}=\{\vect{0}\}\) always. Thus we never have
\(\vect{v}\notin\spn{S}\) in this case.

Now, assume that \(V\) is a nonzero vector space. Write
\(S=\{\vect{v}_1,\dotsc,\vect{v}_m\}\). Then, consider the equation
\begin{equation}
\label{eq:extend-li}
x_1\vect{v}_1+\dotsb+x_m\vect{v}_m+x_{m+1}\vect{v}=0.
\end{equation}
\underline{Case 1}: \(x_{m+1}\ne 0\).

In such case, we can rewrite \Cref{eq:extend-li} as
\[
\vect{v}=-\frac{x_1}{x_{m+1}}\vect{v}_1-\dotsb-\frac{x_m}{x_{m+1}}\vect{v}_m,
\]
meaning that \(\vect{v}\) is a linear combination of vectors in \(S\), thus
\(\vect{v}\in\spn{S}\), contradiction. So case 1 cannot happen.

\underline{Case 2}: \(x_{m+1}=0\).

In this case, we can simplify \Cref{eq:extend-li} as
\[
x_1\vect{v}_1+\dotsb+x_m\vect{v}_m=0,
\]
which implies \(x_1=\dotsb=x_m=0\) by the linear independence of \(S\).
Thus, the only solution to \Cref{eq:extend-li} is
\[
x_1=\dotsb=x_m=x_{m+1}=0,
\]
which means that \(S\cup\{\vect{v}\}\) is linearly independent.
\end{pf}
\item \Cref{thm:add-vec-not-in-spn} suggests how we can add vectors to a
linearly independent set while preserving its linear independence. But only
this is not enough for fully justifying the extension approach. We also need
the following result which guarantees that a basis must be obtained when the
cardinality of set reaches the dimension during the extension process.

\begin{theorem}
\label{thm:card-dim-li-basis}
Let \(m\) be the dimension of a vector space \(V\). If \(S\) is a linearly
independent subset of \(V\) with \(m\) vectors, then \(S\) is a basis for
\(V\).
\end{theorem}
\begin{pf}
Assume to the contrary that \(\spn{S}\ne V\). Then there exists \(\vect{v}\in
V\) such that \(\vect{v}\notin\spn{S}\). Next, using
\Cref{thm:add-vec-not-in-spn}, the set \(S\cup\{\vect{v}\}\) is linearly
independent. This contradicts \Cref{prp:li-num-leq-dim} since its cardinality is
\(|S\cup\{\vect{v}\}|=m+1>m=\dim(V)\)\footnote{When \(\vect{v}\notin\spn{S}\), it
implies in particular that \(\vect{v}\notin S\). Thus, \(|S\cup\{\vect{v}\}|\)
is indeed \(m+1\).}.
\end{pf}

\item Next, we consider theorems that justify \emph{reduction}. Likewise two
theorems are needed for fully justifying the reduction approach.

\begin{theorem}
\label{thm:remove-vec-still-spn}
Let \(S\) be a spanning set of a vector space \(V\). If \(S\) is linearly
\emph{dependent}, then there exists \(\vect{v}\in S\) such that
\(\spn{S\setminus \{\vect{v}\}}=\spn{S}=V\).
\end{theorem}
\begin{pf}
When \(V=\{\vect{0}\}\), this result again holds vacuously since the only
spanning set for \(V\) is the empty set \(\varnothing\), which must be linearly
independent.

Now assume that \(V\) is nonzero. Write \(S=\{\vect{v}_1,\dotsc,\vect{v}_m\}\).
Since \(S\) is linearly dependent, there exist scalars \(a_1,\dotsc,a_m\), not
all zero, such that
\[
a_1\vect{v}_1+\dotsb+a_m\vect{v}_m=0.
\]
Choose an \(i\in\{1,\dotsc,n\}\) with \(a_i\ne 0\). Then rewrite the equation as
\[
\vect{v}_i=-\frac{a_1}{a_i}\vect{v}_1-\dotsb-\frac{a_{i-1}}{a_i}\vect{v}_{i-1}
-\frac{a_{i+1}}{a_i}\vect{v}_{i+1}-\dotsb-\frac{a_m}{a_i}\vect{v}_m.
\]
Thus, \(\vect{v}_i\in\spn{S\setminus \{\vect{v}_i\}}\), and so
\[
\spn{S\setminus \{\vect{v}_i\}}=\spn{S}=V.
\]
\end{pf}

\begin{note}
The proof implicitly suggests that how we can find such vector \(\vect{v}\) to
be removed from the linearly dependent set \(S\). We just pick a vector in
\(S\) that is a linear combination of the other vectors in \(S\), which is
guaranteed to exist by \Cref{thm:remove-vec-still-spn}.
\end{note}
\item Next, we have a result that is analogous to \Cref{thm:card-dim-li-basis}.
But before proving it, we need the following lemma.
\begin{lemma}
\label{lma:basis-subset}
Let \(S\) be a finite subset of a vector space \(V\) with \(m\in\N_0\)
(distinct) nonzero vectors.  Then there exists a subset \(\beta\) of \(S\) such
that \(\beta\) is a basis for \(\spn{S}\).
\end{lemma}
\begin{pf}
We will prove by induction on the cardinality of \(S\): \(m\).

Firstly, the case \(m=0\) trivially holds since in such case we have
\(\spn{S}=\spn{\varnothing}=\{\vect{0}\}\), and thus we can just choose
\(\beta=S=\varnothing\) to be a basis for the zero vector space.

Assume for induction that the case \(m=k\) holds for a \(k\in\N_0\). Now,
consider any finite subset \(S\) of \(V\) with \(k+1\) nonzero vectors.

\underline{Case 1}: \(S\) is linearly independent.

Then we can simply choose \(\beta=S\) to be a basis for \(\spn{S}\).

\underline{Case 2}: \(S\) is linearly dependent.

Since \(S\) is a spanning set for \(\spn{S}\), by
\Cref{thm:remove-vec-still-spn}, there exists \(\vect{v}\in S\) such that
\(\spn{S\setminus \{\vect{v}\}}=\spn{S}\). Applying the induction hypothesis on
the set \(S\setminus \{\vect{v}\}\) (with \(k\) nonzero vectors), there exists
\(\beta\subseteq S\setminus \{\vect{v}\}\subseteq S\) such that \(\beta\) is a
basis for \(\spn{S\setminus \{\vect{v}\}}=\spn{S}\).

Thus, the case \(m=k+1\) holds, and so the result follows by induction.
\end{pf}

\item Now, we can prove the desired theorem.
\begin{theorem}
\label{thm:card-dim-spn-basis}
Let \(m\) be the dimension of a vector space \(V\). If \(S\) is a spanning set
of \(V\) with \(m\) vectors, then \(S\) is a basis for \(V\).
\end{theorem}
\begin{pf}
Assume to the contrary that \(S\) is not a basis for \(V\). Then by
\Cref{lma:basis-subset}, there exists a subset \(\beta\) of \(S\) such that
\(\beta\) is a basis for \(\spn{S}=V\). Since \(S\) is \emph{not} a basis for
\(V\), the subset inclusion must be proper and it must be the case that
\(|\beta|< |S|=m\). This implies that \(\dim(V)=|\beta|<m\), contradiction.
\end{pf}
\end{enumerate}
\subsection{Column Spaces}
\label{subsect:col-sp}
\begin{enumerate}
\item Let \(A\) be an \(m\times n\) matrix and let
\(\vect{v}_1,\dotsc,\vect{v}_n\) be the columns of \(A\). Recall that the
column space of \(A\) is defined as
\(\col{A}=\spn{\{\vect{v}_1,\dotsc,\vect{v}_n\}}\). By
\labelcref{it:col-space-expr}, we can write
\[
\col{A}=\{A\vect{x}\in\R^n:\vect{x}\in\R^m\}.
\]
\item Considering the columns of an \(n\times n\) matrix \(A\), we can obtain
some more criteria for matrix invertibility. The criteria are collected below.

\begin{theorem}
\label{thm:matx-inv-crit}
Let \(A\) be an \(n\times  n\) matrix with columns
\(\vect{v}_1,\dotsc,\vect{v}_n\). Then the following are equivalent.
\begin{enumerate}
\item \(A\) is invertible.
\item \(\det A\ne 0\).
\item For any \(\vect{b}\in\R^n\), \(A\vect{x}=\vect{b}\) has a unique solution.
\item \(\{\vect{v}_1,\dotsc,\vect{v}_n\}\) is a basis for \(\R^n\).
\item \(\{\vect{v}_1,\dotsc,\vect{v}_n\}\) is linearly independent.
\end{enumerate}
\end{theorem}
\begin{pf}
\underline{\(\text{(a)}\iff\text{(b)}\)}: It follows from \Cref{thm:inv-iff-nonzero-det}.

\underline{\(\text{(a)}\iff\text{(c)}\)}: It follows from \Cref{prp:inv-iff-unique-sol}.

\underline{\(\text{(a)}\iff\text{(e)}\)}: Note that
\begin{align*}
&\hspace{1cm}\text{\(\{\vect{v}_1,\dotsc,\vect{v}_n\}\) is linearly independent}\\
&\iff \text{the homogeneous system \(A\vect{x}=\vect{0}\) only has the trivial solution} \\
&\iff \text{\(A\) is invertible\qquad (by \Cref{prp:homo-same-var-eqs-equiv})}.
\end{align*}
It then suffices to prove that \(\text{(d)}\iff\text{(e)}\).

\underline{\(\text{(d)}\implies\text{(e)}\)}: Immediate since a basis must be
linearly independent.

\underline{\(\text{(e)}\implies\text{(d)}\)}: It follows from
\Cref{thm:card-dim-li-basis} since \(\dim(\R^n)=n\).
\end{pf}
\item Next, we would like to find \emph{bases} and \emph{dimensions} of column
spaces. We start by considering the following result about the preservation of
linear independence after multiplication by an invertible matrix.

\begin{theorem}
\label{thm:mult-inv-matx-preserv-li}
Let \(P\) be an invertible \(n\times n\) matrix. Then
\(\{\vect{v}_1,\dotsc,\vect{v}_k\}\) is linearly independent in \(\R^n\) iff
\(\{P\vect{v}_1,\dotsc,P\vect{v}_k\}\) is linearly independent in \(\R^n\).
\end{theorem}
\begin{note}
Here we focus on the vector space \(\R^n\) so that \(P\vect{v}_i\) is
well-defined for every \(i=1,\dotsc,k\).
\end{note}

\begin{pf}
``\(\Rightarrow\)'': Assume that \(\{\vect{v}_1,\dotsc,\vect{v}_k\}\) are
linearly independent. Consider
\[
x_1(P\vect{v}_1)+\dotsb+x_k(P\vect{v}_k)=0
\]
where \(x_1,\dotsc,x_k\in\R\) are unknowns. Since \(P\) is invertible,
we can multiply both sides by \(P^{-1}\) to get
\[
x_1\vect{v}_1+\dotsb+x_k\vect{v}_k=0,
\]
which implies that \(x_1=\dotsb=x_k=0\) by the linear independence of
\(\{\vect{v}_1,\dotsc,\vect{v}_k\}\).

``\(\Leftarrow\)'': Similar to the ``\(\Rightarrow\)'' direction except that we
multiply both sides of the equation by matrix \(P\) instead of \(P^{-1}\).
\end{pf}

\item By definition, we have
\(\col{A}=\spn{\{\vect{v}_1,\dotsc,\vect{v}_n\}}\), so
\(\{\vect{v}_1,\dotsc,\vect{v}_n\}\) is a spanning set for \(\col{A}\), when
considered as a vector space. Then, by the \emph{reduction} approach in
\Cref{subsect:construct-bases}, we know that there is a subset of
\(\{\vect{v}_1,\dotsc,\vect{v}_n\}\) that forms a basis for \(\col{A}\). Here
we will provide a systematic method for obtaining such a basis in
\Cref{thm:find-col-space-basis}. But before that, we need the following
preparatory result, which suggests the preservation of \emph{dimension} of
column space after multiplication by an invertible matrix.

\begin{theorem}
\label{thm:mult-inv-matx-preserv-dim}
Let \(A\) be an \(m\times n\) matrix and \(P\) be an invertible \(m\times m\)
matrix. Then, \(\dim(\col{PA})=\dim(\col{A})\).
\end{theorem}
\begin{pf}
Let \(\vect{v}_1,\dotsc,\vect{v}_n\) be the columns of \(A\). We know that a
subset of \(\{\vect{v}_1,\dotsc,\vect{v}_n\}\) forms a basis for \(\col{A}\),
and we denote it by \(\{\vect{v}_{i_1},\dotsc,\vect{v}_{i_k}\}\). We want to
show that \(\{P\vect{v}_{i_1},\dotsc,P\vect{v}_{i_k}\}\) (having the same
cardinality as \(\{\vect{v}_{i_1},\dotsc,\vect{v}_{i_k}\}\)) is a basis for
\(\col{PA}\), and therefore \(\dim(\col{PA})=\dim(\col{A})\).

Its linear independence follows from \Cref{thm:mult-inv-matx-preserv-li}, so it
suffices to prove that \(\spn{\{P\vect{v}_{i_1},\dotsc,P\vect{v}_{i_k}\}}=\col{PA}\).
Firstly, by block multiplication we can write
\(PA=\mqty[P\vect{v}_1&\cdots&P\vect{v}_n]\), thus
\[
\spn{\{P\vect{v}_1,\dotsc,P\vect{v}_n\}}=\col{PA}.
\]
Since \(\{\vect{v}_{i_1},\dotsc,\vect{v}_{i_k}\}\) is a basis (and thus a
spanning set) for \(\col{A}\), for every \(j=1,\dotsc,n\), we can write
\(\vect{v}_j\) as a linear combination of
\(\vect{v}_{i_1},\dotsc,\vect{v}_{i_k}\), so we can also write \(P\vect{v}_j\)
as a linear combination of \(P\vect{v}_{i_1},\dotsc,P\vect{v}_{i_k}\). This
implies that
\[
\spn{\{P\vect{v}_{i_1},\dotsc,P\vect{v}_{i_k}\}}
=\spn{\{P\vect{v}_1,\dotsc,P\vect{v}_n\}}=\col{PA}.
\]
\end{pf}

\item If the condition that \(P\) is invertible is dropped in
\Cref{thm:mult-inv-matx-preserv-dim}, we would only have a weaker result where
the equality becomes an inequality, as suggested below.

\begin{proposition}
\label{prp:mult-matx-reduce-col-sp-dim}
Let \(A\) be an \(m\times n\) matrix and \(P\) be an \(m\times m\) matrix. Then,
\(\dim(\col{PA})\le\dim(\col{A})\).
\end{proposition}
\begin{pf}
Let \(\vect{v}_1,\dotsc,\vect{v}_n\) be the columns of \(A\), and consider a
basis \(\{\vect{v}_{i_1},\dotsc,\vect{v}_{i_k}\}\) for \(\col{A}\). Using the
same argument as the proof for \Cref{thm:mult-inv-matx-preserv-dim}, we can
show that \(S=\{P\vect{v}_{i_1},\dotsc,P\vect{v}_{i_k}\}\) is a spanning set for
\(\col{PA}\), when considered as a vector space.

Then, by the \emph{reduction} approach in \Cref{subsect:construct-bases}, a
basis for \(\col{PA}\) can be obtained by removing vectors in the spanning set
\(S\), if necessary. This implies that \(\dim(\col{PA})\le |S|=\dim(\col{A})\).
\end{pf}

\item Now, we give the systematic method for obtaining a basis for column space
as follows.
\begin{theorem}
\label{thm:find-col-space-basis}
Let \(A\) be an \(m\times n\) matrix and \(R\) be the RREF of \(A\). Then, the
columns of \(A\) corresponding to the columns containing the leading ones in
\(R\) form a basis for \(\col{A}\).
\end{theorem}
\begin{pf}
Let \(\vect{v}_1,\dotsc,\vect{v}_n\) be the columns of \(A\). By Gaussian
elimination, \(R=E_r\dotsb E_1A\) for some elementary matrices
\(E_1,\dotsc,E_r\). Let \(E=E_r\dotsb E_1\). Then we can write
\[
R=EA=\mqty[E\vect{v}_1&\cdots E\vect{v}_n]
\]
by block multiplication.

Denote the columns in \(R\) with leading ones by
\(E\vect{v}_{i_1},\dotsc,E\vect{v}_{i_k}\). It is not hard to see that the
homogeneous system
\[
\mqty[E\vect{v}_{i_1}&\cdots&E\vect{v}_{i_k}]\vect{x}=\vect{0}
\]
has only the trivial solution, by considering the definition of RREF.  So,
\(\{E\vect{v}_{i_1},\dotsc,E\vect{v}_{i_k}\}\) is linearly independent.

By the definition of RREF, all entries in the rows of \(R\) without leading
ones are zero. Thus, every column of \(R\) that does not contain any leading
one in \(R\) can be expressed as a linear combination of
\(E\vect{v}_{i_1},\dotsc,E\vect{v}_{i_n}\). Consequently,
\[
\col{R}=\spn{\{E\vect{v}_{i_1},\dotsc,E\vect{v}_{i_k}\}}.
\]
So, we know that \(\{E\vect{v}_{i_1},\dotsc,E\vect{v}_{i_k}\}\) is a basis for
\(\col{R}\), thus \(\dim(\col{R})=k\).

Since \(E\) is invertible, we know that
\(\{\vect{v}_{i_1},\dotsc,\vect{v}_{i_k}\}\) is linearly independent by
\Cref{thm:mult-inv-matx-preserv-li}, and
\(\dim(\col{A})=\dim(\col{EA})=\dim(\col{R})=k\) by
\Cref{thm:mult-inv-matx-preserv-dim}. Thus,
\(\{\vect{v}_{i_1},\dotsc,\vect{v}_{i_k}\}\) is a basis for \(\col{A}\) by
\Cref{thm:card-dim-li-basis}.
\end{pf}

\begin{corollary}
\label{cor:dim-col-space-num-lead-ones}
Let \(A\) be an \(m\times n\) matrix. Then \(\dim(\col{A})\) is the number of
leading ones in the RREF of \(A\).
\end{corollary}
\begin{pf}
It follows from \Cref{thm:find-col-space-basis}, by noting the number of
columns of \(A\) corresponding to the leading ones in \(R\) is just the number
of leading ones in \(R\), where \(R\) is the RREF of \(A\).
\end{pf}
\end{enumerate}
\subsection{Row Spaces}
\begin{enumerate}
\item After discussing column space, we will discuss \emph{row space}. Let
\(A\) be an \(m\times n\) matrix with \emph{rows}
\(\vect{u}_1,\dotsc,\vect{u}_m\) (as row vectors). The \defn{row space} of
\(A\) is defined as \(\row{A}=\spn{\{\vect{u}_1,\dotsc,\vect{u}_m\}}\).
Likewise, we would like to find bases and dimensions of row spaces.

\item The first result suggests that multiplication of an invertible matrix
preserves the whole row space (not just the dimension!).

\begin{theorem}
\label{thm:mult-inv-matx-preserv-row-space}
Let \(A\) be an \(m\times n\) matrix and \(P\) be an invertible \(m\times m\)
matrix. Then, \(\row{PA}=\row{A}\).
\end{theorem}
\begin{pf}
Let \(\vect{u}_1,\dotsc,\vect{u}_m\) be the rows of \(A\), and
\(\vect{w}_1,\dotsc,\vect{w}_m\) be the rows of \(PA\). Write \(P=[p_{ij}]\).
Then, by block multiplication, we have
\[
PA=\mqty[p_{11}&\cdots&p_{1m}\\ \vdots&\ddots&\vdots\\ p_{m1}&\cdots&p_{mm}]
\mqty[\vect{u}_1\\ \vdots\\ \vect{u}_m]
=\mqty[
p_{11}\vect{u}_1+\dotsb+p_{1m}\vect{u}_m\\
\vdots\\
p_{m1}\vect{u}_1+\dotsb+p_{mm}\vect{u}_m
]
\]
This shows that for every row \(w_i\) of \(PA\) is a linear combination of
\(\vect{u}_1,\dotsc,\vect{u}_m\). Hence every linear combination of
\(\vect{w}_1,\dotsc,\vect{w}_m\) is also a linear combination of
\(\vect{u}_1,\dotsc,\vect{u}_m\). It follows that
\[
\row{PA}=\spn{\{\vect{w}_1,\dotsc,\vect{w}_m\}}\subseteq
\spn{\{\vect{u}_1,\dotsc,\vect{u}_m\}}=\row{A}.
\]

So now we have proved that \(\row{QB}\subseteq\row{B}\) for any \(m\times n\)
matrix \(B\) and \(m\times m\) (invertible) matrix \(Q\). By setting \(B=PA\)
and \(Q=P^{-1}\), we get
\[
\row{A}=\row{P^{-1}(\vc{PA})}\subseteq \row{\vc{PA}},
\]
so the result follows.
\end{pf}

\item If the condition that \(P\) is invertible is dropped in
\Cref{thm:mult-inv-matx-preserv-row-space}, we would only have a weaker result
about \emph{inequality} of \emph{dimensions}, as suggested below.

\begin{proposition}
\label{prp:mult-matx-reduce-row-sp-dim}
Let \(A\) be an \(m\times n\) matrix and \(P\) be an \(m\times m\) matrix. Then,
\(\dim(\row{PA})\le\dim(\row{A})\).
\end{proposition}
\begin{pf}
Using the same argument as the first part of the proof for
\Cref{thm:mult-inv-matx-preserv-row-space}, we can show that
\(\row{PA}\subseteq \row{A}\), thus \(\row{PA}\) is a vector subspace of
\(\row{A}\), even without the assumption that \(P\) is invertible. Then, by
\labelcref{it:dim-subspace}, we have \(\dim(\row{PA})\le\dim(\row{A})\).
\end{pf}

\item The next result suggests how we can find a basis for row space.
\begin{theorem}
\label{thm:nonzero-rows-basis-row-sp}
Let \(A\) be an \(m\times n\) matrix and \(R\) be the RREF of \(A\). Let
\(\vect{w}_1,\dotsc,\vect{w}_k\) be the non-zero rows in the RREF of \(A\).
Then, \(\{\vect{w}_1,\dotsc,\vect{w}_k\}\) is a basis for \(\row{A}\).
\end{theorem}
\begin{pf}
First of all, by Gaussian elimination, we can write \(R=E_r\dotsb E_1A\) for
some elementary matrices \(E_1,\dotsc,E_r\). Write \(E=E_r\dotsb E_1\) and note
that it is invertible. Thus, by \Cref{thm:mult-inv-matx-preserv-row-space},
\(\row{R}=\row{EA}=\row{A}\), so it suffices to show that
\(\{\vect{w}_1,\dotsc,\vect{w}_k\}\) is a basis for \(\row{R}\).

Since adding zero (row) vectors into the set
\(\{\vect{w}_1,\dotsc,\vect{w}_k\}\) does not change its span, we see that\\
\(\spn{\{\vect{w}_1,\dotsc,\vect{w}_k\}}=\row{R}\). Also, by the definition of
RREF, the nonzero rows contain the leading ones at different positions. From
this we can observe that \(\{\vect{w}_1,\dotsc,\vect{w}_k\}\) is linearly
independent. Thus, \(\{\vect{w}_1,\dotsc,\vect{w}_k\}\) is a basis for
\(\row{R}\), as desired.
\end{pf}
\begin{corollary}
\label{cor:dim-row-space-num-lead-ones}
Let \(A\) be an \(m\times n\) matrix. Then \(\dim(\row{A})\) is the number of
leading ones in the RREF of \(A\).
\end{corollary}
\begin{pf}
It follows from \Cref{thm:nonzero-rows-basis-row-sp}, by noting the number of
nonzero rows in \(R\) is just the number of leading ones in \(R\), where \(R\)
is the RREF of \(A\).
\end{pf}
\end{enumerate}
\subsection{Matrix Rank}
\begin{enumerate}
\item A fundamental result in linear algebra that connects row and column
spaces is the following. It suggests that row and column spaces of the same
matrix must always have the same dimension.
\begin{theorem}
\label{thm:row-col-same-dim}
Let \(A\) be an \(m\times n\) matrix. Then \(\dim(\row{A})=\dim(\col{A})\).
\end{theorem}
\begin{pf}
Let \(R\) be the RREF of \(A\). Then, we can write \(R=E_r\dotsb E_1A\) for
some elementary matrices \(E_1,\dotsc,E_r\). Let \(E=E_r\dotsb E_1\), and note
that \(E\) is invertible. Thus, by
\Cref{thm:mult-inv-matx-preserv-dim,thm:mult-inv-matx-preserv-row-space}, we
have
\[
\dim(\col{A})=\dim(\col{R})
\]
and
\[
\dim(\row{A})=\dim(\row{R})
\]
By \Cref{cor:dim-col-space-num-lead-ones,cor:dim-row-space-num-lead-ones}, we
then have
\[
\dim(\col{R})=\dim(\row{R})=\text{number of leading ones in \(R\)},
\]
which implies that \(\dim(\row{A})=\dim(\col{A})\).
\end{pf}

\item The common value of the dimensions of row and column spaces of a matrix
is said to be the rank of that matrix. Symbolically, the \defn{rank} of an
\(m\times n\) matrix \(A\) is \(\rk{A}=\dim(\col{A})\;(=\dim(\row{A}))\).

\item Since taking matrix transpose is essentially just interchanging rows and
columns, it does not change the rank.

\begin{corollary}
\label{cor:matx-trans-same-rk}
Let \(A\) be an \(m\times n\) matrix. Then \(\rk{A}=\rk{A^{T}}\).
\end{corollary}
\begin{pf}
Note that \(\rk{A}=\dim(\col{A})=\dim(\row{A^{T}})=\rk{A^{T}}\).
\end{pf}

\item Intuitively, rank measures the ``magnitude'' of non-invertibility of a
matrix. The \emph{lower} the rank of a matrix is, the ``\emph{more}
non-invertible'' the matrix is. Examples:
\begin{itemize}
\item The rank of every zero matrix is \(0\) (the lowest possible rank), so
zero matrix is ``very non-invertible''.
\item The rank of every invertible \(n\times n\) matrix is \(n\) (the highest
possible rank for an \(n\times n\) matrix).

\begin{pf}
Let \(A\) be any invertible matrix. By \Cref{prp:inv-iff-rref-identity}, the
RREF of \(A\) is \(I_n\), so there exist elementary matrices \(E_1,\dotsc,E_r\)
such that \(I_n=E_r\dotsb E_1 A\). Let \(E=E_r\dotsb E_1\), which is
invertible. Then, we have
\[
\rk{A}=\dim(\col{A})
=\dim(\col{EA})
=\dim(\col{I_n})
=\dim(\R^n)
=n.
\]
\end{pf}

Hence, as expected, an \emph{invertible} matrix has no degree of
non-invertibility.
\item The \(3\times 3\) matrix \(\mqty[1&0&1\\ 0&1&1\\ 0&1&1]\) has rank \(2\)
since its RREF is \(\mqty[1&0&1\\ 0&1&1\\ 0&0&0]\), which has two nonzero rows.
While this matrix is not invertible, its rank suggests that its ``degree of
non-invertibility'' is not very high.
\end{itemize}
\end{enumerate}
\subsection{Null Spaces}
\label{subsect:null-sp}
\begin{enumerate}
\item So far we have introduced two kinds of spaces that are related to a
matrix: column and row spaces. We will introduce the third kind here, which is
about the concept of the system of linear equations.

\item Let \(A\) be an \(m\times n\) matrix. Then the \defn{null space} of \(A\)
is given by \(\nul{A}=\{\vect{x}\in\R^n:A\vect{x}=\vect{0}\}\), i.e., the
solution set of the homogeneous system \(A\vect{x}=\vect{0}\).

\item The following result suggests that null space is indeed a vector
subspace of \(\R^n\).

\begin{theorem}
\label{thm:null-sp-vec-subsp}
Let \(A\) be an \(m\times n\) matrix. Then, the null space \(\nul{A}\) is a
vector subspace of \(\R^n\).
\end{theorem}
\begin{pf}
Fix any \(\vect{u},\vect{v}\in \nul{A}\). Then, \(A\vect{u}=A\vect{v}=\vect{0}\).

\underline{Closed under addition}: We have
\(A(\vect{u}+\vect{v})=A\vect{u}+A\vect{v}=\vect{0}+\vect{0}=\vect{0},\) thus
\(\vect{u}+\vect{v}\in\nul{A}\).

\underline{Closed under scalar multiplication}: For any \(a\in\R\),
\(A(a\vect{v})=aA\vect{v}=a\vect{0}=\vect{0},\) thus \(a\vect{v}\in\nul{A}\).
\end{pf}

\item Like column and row spaces, we would like to find dimensions and bases of
null spaces. They can be found as suggested by the following result.

\begin{theorem}
\label{thm:null-sp-dim-basis}
Let \(A\) be an \(m\times n\) matrix and \(R\) be the RREF of \(A\).
\begin{enumerate}
\item The dimension \(\dim(\nul{A})\), called the \defn{nullity} of
\(A\) and denoted by \(\nulty{A}\), equals the number of free variables
in the system \(A\vect{x}=\vect{0}\), or equivalently, \(R\vect{x}=\vect{0}\).

\item Let \(s_1,\dotsc,s_r\) be the parameters assigned to the free variables.
Express the null space of \(A\), i.e., the solution set of the homogeneous
system \(A\vect{x}=\vect{0}\), as
\(\{s_1\vect{v}_1+\dotsb+s_r\vect{v}_r:s_1,\dotsc,s_r\in\R\}\), where
\(\vect{v}_1,\dotsc,\vect{v}_r\in\R^n\).\footnote{This is always possible for a
\emph{homogeneous} system. Each solution in the general solution is a vector
involving free variables \(s_1,\dotsc,s_r\) \emph{without} constant term due to
the homogeneity. Then, each general solution can be ``split'' into
\(s_1\vect{v}_1,\dotsc,s_r\vect{v}_r\).} Then, the set
\(\{\vect{v}_1,\dotsc,\vect{v}_r\}\) is a basis for \(\nul{A}\).
\end{enumerate}
\end{theorem}
\begin{pf}
Note that (b) implies (a) by the definition of dimension. So it suffices to
prove (b).

Firstly, we observe from the above expression of null space that
\[
\nul{A}=\{s_1\vect{v}_1+\dotsb+s_r\vect{v}_r:s_1,\dotsc,s_r\in\R\}
=\spn{\{\vect{v}_1,\dotsc,\vect{v}_r\}}.
\]
This shows \(\{\vect{v}_1,\dotsc,\vect{v}_r\}\) spans \(\nul{A}\).

Next, note that for every \(i=1,\dotsc,r\), the vector \(\vect{v}_i\) has (i) a
nonzero entry at the position for the corresponding free variable in each
solution from the solution set, and (ii) zero entries at the positions for the
rest of the free variables. Thus, \(\{\vect{v}_1,\dotsc,\vect{v}_r\}\) is
linearly independent.
\end{pf}
\item Example: Let \(A=\mqty[1&1&1&1]\). Then the solution set of
\(A\vect{x}=\vect{0}\) is
\(\nul{A}=\{(x_1,x_2,x_3,x_4)^{T}:x_1+x_2+x_3+x_4=0\}\). We can write
\[
\nul{A}=\qty{\mqty[-s-t-u\\ s\\ t\\ u]\in\R^4:s,t,u\in\R}
=\qty{s\mqty[-1\\ 1\\ 0\\ 0]+t\mqty[-1\\ 0\\ 1\\ 0]+u\mqty[-1\\ 0\\ 0\\ 1]:s,t,u\in\R},
\]
thus a basis for \(\nul{A}\) is \(\qty{\mqty[-1\\ 1\\ 0\\ 0], \mqty[-1\\ 0\\
1\\ 0], \mqty[-1\\ 0\\ 0\\ 1]}\).
\end{enumerate}
